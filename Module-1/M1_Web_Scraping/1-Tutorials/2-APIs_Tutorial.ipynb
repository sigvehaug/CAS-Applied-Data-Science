{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-APIs_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigvehaug/CAS-Applied-Data-Science/blob/master/Module-1/M1_Web_Scraping/1-Tutorials/2-APIs_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwwx6YFTV9Vt"
      },
      "source": [
        "# Working with APIs #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id0wT-21WEct"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "<font color='red'>\n",
        "Stars* mark more advanced (or less crucial) topics. <font color = 4e1585>SIDENOTES <font color='red'>provide background information for those who are interested in what happens \"behind the scenes\". \n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEa2mx7XBdfK"
      },
      "source": [
        "In the previous part, we learned how to automatically retrieve pages from the internet through **web scraping**. We sent HTTP requests with the  ``requests`` library and used the ``BeautifulSoup`` library to parse and work with the HTML code from the response we got. For many of the more popular websites such as Wikipedia, Youtube, Twitter or many newspapers, there is a more direct way to retrieve the information we need: through so-called **APIs**. In this tutorial we will learn how to gather web data through APIs. We will use the Wikipedia API to illustrate the process.\n",
        "\n",
        "To follow allong with the tutorial, you will have to import the ``requests`` and the ``BeautifulSoup`` library:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7h726lofAAH"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTIe5VIL3feT"
      },
      "source": [
        "##Getting help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfSxfzDpTjRV"
      },
      "source": [
        "We are selective in this tutorial and only discuss elements that we believe are most important for the purpose of this class. If you want more details, you can consult, for example, the **Python Standard Library Reference** at https://docs.python.org/3/library/ or the **Language Reference** at https://docs.python.org/3/reference/. But be warned: the amount of detail in these sources can be overwhelming. For **quick and easy-to-understand overviews** of different topics see, for example, https://www.w3schools.com/python/. Here are some specific references for today's tutorial:\n",
        "\n",
        "Working with APIs in Python:\n",
        "* https://realpython.com/python-api/\n",
        "* https://www.dataquest.io/blog/python-api-tutorial/\n",
        "\n",
        "JSON: \n",
        "*  https://www.w3schools.com/js/js_json_intro.asp\n",
        "\n",
        "If you get stuck or don't remember how to do something, it is usually a good idea to **Google** your problem. Python has a large (and fast-growing) community and you will probably find answers to most of your questions online (e.g. on **Stack Overflow** or in a **Youtube tutorial**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTTxqag08CcA"
      },
      "source": [
        "## What is an API?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFIjdUC8Lsa_"
      },
      "source": [
        "Suppose you have a (large) list of addresses and need the respective coordinates. If you wanted to do this manually, you could use Google Maps through your browser. But how could you automate this? You could try to generate the URLs for each request and fetch the coordinates from the pages. This may be feasable, but the pages you will request contain a lot of information and graphical \"overhead\" you are not interested in, making the process very tedious and inefficient (and web scrapers often get blocked by such services, as they are mostly comercial or have limited capacity). Fortunately, many providers offer an easier way to access the information in their databases: through APIs (e.g. tha Google Maps API)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLf9nocWQGq_"
      },
      "source": [
        "So what is an API? API stands for **Application Programming Interface** and it is a very broad concept. You can think of it as an interface that is not meant for humans but for computer programs or applications. APIs allow different applications or programs to interact with each other. For example, the different programs installed on your computer may communicate with the operating system of your computer through APIs (e.g. when saving a file)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNcOUGixDxPU"
      },
      "source": [
        "  \n",
        "In our context of gathering data from the web, we are mostly dealing with so-called **Web APIs**. They allow you to access content in a more **structured** way than scraping HTML pages. Web APIs consist of **a set of URLs through which the data is made accessible so that other applications/programs (for example your Python script) can work with it conveniently**. They also allow to take more control over the content you will receive. When providers offer an API, it just means that they’ve built a set of URLs that return structured data — meaning the responses won’t contain the presentational overhead that is required for a graphical user interface like a website. Web APIs have become so important that many people just say API when they are referring to a Web API.\n",
        "\n",
        "This probably still sounds very abstract. We'll walk you through some examples with the Wikipedia API to give you a more concrete idea of how APIs work. That being said, we still download and process data via an URL, only the URL and data structure will change a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7JJyVLCNVo"
      },
      "source": [
        "## Making API requests with the ``requests`` library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKXhsS2cdG8u"
      },
      "source": [
        "How can we gather data through an API? In practice, **retrieving data through an API is quite similar to retrieving data through web scraping**. You will also have to **figure out the correct URL** for your request. In fact, you can type this URL into the browser — **instead of a designed web page, you will directly see structured data**. For example, this URL takes you to the structured data from the cat article on Wikipedia: https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neIjGVGNf9hq"
      },
      "source": [
        "If we want to request the cat article through the API, we can thus just type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O2xXujdgIEx"
      },
      "source": [
        "r = requests.get(\"https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json\")\n",
        "r.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iYQsKhiZUK"
      },
      "source": [
        "Let's take a closer look at the URL we used: The first part of the URL (https://en.wikipedia.org/w/api.php) is an **endpoint** of the API. You can think of endpoints as the base URLs of the API. After the ``?`` we have the **query string** (as we know already from the web scraping tutorial). It specifies the action we want to perform. Usually, it is more convenient to enter the parameters for the query string as an argument. The **``params`` parameter of the ``get`` function** allows you to do this (the order of the parameters typically does not matter):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zer5lYNk4P-"
      },
      "source": [
        "ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "PARAMS = {\n",
        "    \"action\": \"parse\",\n",
        "    \"page\": \"Cat\",\n",
        "    \"format\": \"json\",\n",
        "}\n",
        "\n",
        "r = requests.get(url=ENDPOINT, params=PARAMS)\n",
        "r.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzoci3rgmrWe"
      },
      "source": [
        "To see that this makes exactly the same request, you can take a look the URL we just called:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Z6Zs-vmwX_"
      },
      "source": [
        "r.url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1jFXiM_gZb5"
      },
      "source": [
        "So far, so good. But how did we figure out that https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json was the URL we needed to request? Many APIs have an extensive **documentation** that explain how to make different types of requests. These documentations can sometimes be overwhelming and it is usually a good idea to try to find some examples for the type of request you want to make (e.g. on Stack Overflow or within the documentation). Let's have a look at the documentation for the Wikipedia API: \n",
        "\n",
        "* https://www.mediawiki.org/wiki/API:Main_page\n",
        "\n",
        "We can see that there are different endpoints. For example, there is one endpoint for each language. The endpoint for English Wikipedia is: \n",
        "* https://en.wikipedia.org/w/api.php \n",
        "\n",
        "Now we will have to figure out what parameters to choose (i.e. how to construct the query string). If you scroll down on the page, you will see that there are many options as to how the parameters could be set. If you click through the documentation (or Google something like \"parse page Wikipedia API\") you may eventually get to this page that provides some useful examples as to how the content of a page could be retrieved: \n",
        "* https://www.mediawiki.org/wiki/API:Parsing_wikitext\n",
        "\n",
        "There, you will find example code similar to the one we used above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmxIC_0odgEI"
      },
      "source": [
        "><font color = 4e1585> SIDENOTE: APIs often allow you to do many different things. You can retrieve information, but sometimes you can also add, change or delete information. Many Web APIS are so-called RESTful APIs, meaning that they implement the standard HTTP methods: GET, POST, PUT and DELETE. For data science projects, you are typically only interested in GET requests (as you don't want to change anything on the respective websites). </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq5A6gPV8HAu"
      },
      "source": [
        "## Working with JSON data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrES187T2RDH"
      },
      "source": [
        "We mentioned that APIs allow you to access *structured data*. What exactly does this mean? Typically, the data you will retrieve through an API will be formatted as **JSON**. JSON stands for JavaScript Object Notation; it is a data exchange format based on human-readable text (as opposed to binary data formats) that is commonly used for data transportation over the web. Let's take a look at some JSON code:\n",
        "\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"Peter\",\n",
        "  \"age\": 56,\n",
        "  \"married\": false,\n",
        "  \"children\": [\n",
        "    {\n",
        "      \"name\": \"Nina\",\n",
        "      \"age\": 19,\n",
        "      \"educational degree\": \"High school\"\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"Mary\",\n",
        "      \"age\": 14,\n",
        "      \"educational degree\": null\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej-SXXBEOcD3"
      },
      "source": [
        "><font color = 4e1585> SIDENOTE: If you want to know if something is valid (correctly formatted) JSON you can search for 'JSON lint' or 'JSON linter' and use one of the online tools, e.g. https://jsonlint.com. A lint or linter is a tool to check for errors in code (see https://en.wikipedia.org/wiki/Lint_(software) for more details)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxWYiHmm3trK"
      },
      "source": [
        "You may have noticed that JSON does not look so different from a Python dictionary. The great thing about JSON is that it can easily be **converted into a (nested) Python dictionary**. Let's parse the response we got from our Wikipida API request:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMUBPFSW3RBJ"
      },
      "source": [
        "data = r.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5zeTd8_5mqv"
      },
      "source": [
        "print(data)\n",
        "type(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ0o49Hs5pU4"
      },
      "source": [
        "We have now created a (large) Python dictionary. How can we **access the different parts** of it? If you **type the URL of your request into your browser**, you can expore the nested structure of the JSON code (and thus of your Python dictionary):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBFqFa8e-s0e"
      },
      "source": [
        "r.url # this will display the URL; copy the URL to your browser and see what happens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtmMGBVU-z2E"
      },
      "source": [
        "\n",
        "Similarly, it is usually a good idea to explore the **keys** of the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZyjF7ZR60-6"
      },
      "source": [
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIxXtvYO64fl"
      },
      "source": [
        "We only have one key at the top level (``parse``). Let's explore its value to see what we have on the next level:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2x4SwQP7OoF"
      },
      "source": [
        "data[\"parse\"].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLJ5eGwV7U_5"
      },
      "source": [
        "Now we can access different types of information about the page. Let's access the title:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlqrKJXt7zTR"
      },
      "source": [
        "data[\"parse\"][\"title\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP09G8LT8N0H"
      },
      "source": [
        "Let's now try to access the sections:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpKjIZvg79XG"
      },
      "source": [
        "sections = data[\"parse\"][\"sections\"]\n",
        "sections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyDh_DSl8frc"
      },
      "source": [
        "We got a list of dictionaries containing information on each section. If we are only interested in the heading of each section, we could write a list comprehension to extract it. The \"line\" key in each dictionary appears to contain the section heading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVory0Zl81nj"
      },
      "source": [
        "[section[\"line\"] for section in sections]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzEfQFsn9XS3"
      },
      "source": [
        "In this way, you can access different parts of your dictionary and extract the information on the page (e.g. internal and external links, images etc.). The dictionary (within \"parse\") **also contains a \"text\" key where the HTML code of the article is stored** (within the \"*\" key). You can convert it to a BeautifulSoup object and search it based on HTML tags (see notebook on web scraping): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do3PxVSl-2-z"
      },
      "source": [
        "# Get html text \n",
        "html_text = BeautifulSoup(data[\"parse\"][\"text\"][\"*\"])\n",
        "type(html_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zouIBKCs_eWA"
      },
      "source": [
        "# get opening paragraph\n",
        "html_text.find_all(\"p\")[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt07LNudBE8p"
      },
      "source": [
        "---\n",
        "\n",
        ">  <font color='teal'> **In-class exercise**:\n",
        "Request the *Dog* page on Wikipedia (formatted as JSON) through the API and convert the response to a Python dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkq6nK-LzPHF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBzARxtjzmzZ"
      },
      "source": [
        ">  <font color='teal'> Print the title and all links to other Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iSEvtl45xC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luyghogH7T28"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEMlhE3PzPeW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqoiHSlKTADW"
      },
      "source": [
        "## Making advanced requests with the Wikipedia API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYnRJp_P_92w"
      },
      "source": [
        "The Wikipedia API allows you to do much more than just retrieving a single page. For example, you can search for **pages that match with certain words** (full text search in title or content: https://www.mediawiki.org/wiki/API:Search). Let's retrieve all pages about cats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KK6cH0RFq4r"
      },
      "source": [
        "# Perform page search \n",
        "URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "PARAMS = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"search\",\n",
        "    \"srsearch\": \"cat\",\n",
        "    \"srlimit\": \"max\"\n",
        "}\n",
        "\n",
        "r = requests.get(url=URL, params=PARAMS)\n",
        "\n",
        "# Convert to dictionary\n",
        "DATA = r.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQ4zUrAyBcF"
      },
      "source": [
        "DATA.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA0Eui7wGT9Z"
      },
      "source": [
        "# Navigate through dictionary\n",
        "print(DATA.keys())\n",
        "print(DATA[\"query\"].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT99YJukF1EV"
      },
      "source": [
        "# Get titles of all cat pages that were found\n",
        "pages = DATA[\"query\"][\"search\"]\n",
        "cat_pages = [page[\"title\"] for page in pages]\n",
        "print(cat_pages) \n",
        "len(cat_pages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1v3xdNu5YZR"
      },
      "source": [
        "Another interesting thing you can do is to **find pages based on coordinate locations** (see: https://www.mediawiki.org/wiki/API:Geosearch). Let's find all Wikipedia pages of places that are close (<1km) to the main building of University of Bern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BsM2oXTKZz5"
      },
      "source": [
        "# Perform page search \n",
        "URL = \"https://de.wikipedia.org/w/api.php\"  # Change to German Wikipedia!\n",
        "\n",
        "PARAMS = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"geosearch\",\n",
        "    \"gscoord\": \"46.950519|7.438109\",\n",
        "    \"gslimit\": \"max\",\n",
        "    \"gsradius\": 1000 \n",
        "}\n",
        "\n",
        "r = requests.get(url=URL, params=PARAMS)\n",
        "\n",
        "# Convert to dictionary\n",
        "DATA = r.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12JL1sEMtkT"
      },
      "source": [
        "PLACES = DATA['query']['geosearch']\n",
        "[place[\"title\"] for place in PLACES]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVUHkdRcNsYF"
      },
      "source": [
        "There are many more things you can do with the Wikipedia API. Some of them would be extremely difficult (or impossible) to achieve through web scraping (e.g. the two examples in this section). Check out the documentation (or Google your idea) to find out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI2tCnawRagq"
      },
      "source": [
        "## Using the ``wikipedia`` module for simple requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FxBzABtOAzL"
      },
      "source": [
        "For many of the more popular APIs, someone has written a **Python Module** that allows you to work with the API more conveniently. For example, there is a **``wikipedia`` module** to work with the Wikipedia API (see here for a documentation: https://pypi.org/project/wikipedia/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMTcRwNZPCmv"
      },
      "source": [
        "# install and import wikipedia module\n",
        "!pip install wikipedia # When ! is being added at the beginning the statement is performed in the command-line!\n",
        "import wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu5ryId6PF2z"
      },
      "source": [
        "# Search for pages about cats\n",
        "wikipedia.search(\"Cat\") # only first 10 results by default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrf87_VQagL"
      },
      "source": [
        "# Get contents of cat page\n",
        "wikipedia.page('Cat', auto_suggest=False).content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSPOb7MwRLCk"
      },
      "source": [
        "Such modules can be very convenient if you want to perform simple requests. However, they are often not very flexible and may not always allow you to make the requests you would like to perform. <font color='red'>Moreover, the quality may vary (anyone can program and upload such a module) as the following example shows:</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFmkv4fYiAVW"
      },
      "source": [
        "wikipedia.page('Cat').content # Search for 'Cat' shows result for 'Hat', as auto_suggest parameter is True by default."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2mB0tC5WLfU"
      },
      "source": [
        "---\n",
        "\n",
        ">  <font color='teal'> **In-class exercise**:\n",
        "Retrieve 20 Wikipedia pages that are located within a 2 km radius from the main train station in Bern (46.949722°, 7.439444°)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zen6VFiWP1F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8KlRXWR1ZTH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUEJZTQ4WQ3N"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85yXHaH-X6Lx"
      },
      "source": [
        "## Working with APIs in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URLakgjKUJA-"
      },
      "source": [
        "Now we have seen many different ways to retrieve data from the web and you might be asking yourself: When should I do web scraping? When should I use an API? When should I use a Python module (that accesses an API)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlawQ0stVXQw"
      },
      "source": [
        "Or course, this depends on your project and on your preferences, but here's a rule of thumb: \n",
        "\n",
        "* Whenever there is an **API**, it is usually easier to work with it than to scrape the pages. Also, some data is only really accessible through APIs (e.g. if you want to retrieve all tweets about a certain topic on Twitter). So if there's an API, it's usually (but of course not always) a good idea to use it!\n",
        "\n",
        "* If there's a **Python library** for the API, it is a good idea to check it out and see what it can do. If it suits your needs, it may save you a lot of work (reading the API documentation etc.). However, some people prefer to work directly with the API to keep full control over their implementation.\n",
        "\n",
        "* **Web scraping** is useful when there is no API, which is the case for most websites. For some tasks, you may also find it easier to just scrape the pages instead of trying to find out how the API works.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_86BDYZ2A3z6"
      },
      "source": [
        "When you start working with other APIs you will notice that each one works a bit differently. For example, many APIs require **authentication** (e.g. Twitter API, Google Maps API, Facebook API etc.). This means that you will first have to create a \"developer account\" and will then get some credentials you have to include into your request. You will have to read the documentation of the API (or some blog or answer on Stack Overflow) to find out how this works for the API you are interested in. Also, not all APIs give you **free access** to all the data. For example, the Google Maps API will start to charge you a price after a certain amount of requests, so be careful with your implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sESJcMRBqd1V"
      },
      "source": [
        "APIs are becoming increasingly common and larger providers usually have one (or several). Here are some **APIs you may find useful for a research project**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9mCCAFysbM7"
      },
      "source": [
        "| Name | Link | Python Module(s) |\n",
        "| --- | --- | --- |\n",
        "| Wikipedia API | https://www.mediawiki.org/wiki/API:Main_page | wikipedia, Wikipedia-API |\n",
        "| Twitter API | https://developer.twitter.com/en/docs/twitter-api | twitter, tweepy |\n",
        " Google Maps API| https://developers.google.com/maps/documentation | python-gmaps|\n",
        "| LinkedIn API|https://www.linkedin.com/developers/|python-linkedin-v2|\n",
        "| New York Times API|https://developer.nytimes.com/apis| pynytimes|\n",
        "| The Guardian API |https://open-platform.theguardian.com/documentation/|| \n",
        "| COVID-19 API |https://covid19api.com/ |\n",
        "| Google Trends (no official API) | https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f | pytrends|\n",
        "\n"
      ]
    }
  ]
}