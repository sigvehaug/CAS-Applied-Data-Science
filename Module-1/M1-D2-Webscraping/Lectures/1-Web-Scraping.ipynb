{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["AC0IV5rBNJn-","H2MySC5fIDmJ"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Gwwx6YFTV9Vt"},"source":["# Web Scraping #\n","#### CAS Applied Data Science 2025 ####"]},{"cell_type":"markdown","metadata":{"id":"pdPiOhvvVa5l"},"source":["In today's tutorial, we will learn how to collect our own data from the internet through **web scraping** and **APIs**. This is a very broad topic that relates to many technologies you may not know about yet. Don't get discouraged if you encounter many things that are new for you. Even if you do not fully understand how everything works in detail, you will soon be able to extract data from the internet and work with it – and over time, you will develop a better understanding of the things that happen behind the scenes!\n","\n","In this tutorial, we will only be able to scratch the surface and cover the most important concepts. However, there are many helpful books and online resources if you would like to dive deeper into the topic."]},{"cell_type":"markdown","metadata":{"id":"AshYEIHVOsEP"},"source":["## Getting help"]},{"cell_type":"markdown","metadata":{"id":"ekpUsERVKiWO"},"source":["We are selective in this tutorial and only discuss elements that we believe are most important to get started with aquiring data from the web. If you want more details, you can consult, for example, the **Python Standard Library Reference** at https://docs.python.org/3/library/ or the **Language Reference** at https://docs.python.org/3/reference/. But be warned: the amount of detail in these sources can be overwhelming. For **quick and easy-to-understand overviews** of different topics see, for example, https://www.w3schools.com/python/. For comprehensive information about web technologies visit https://developer.mozilla.org/. Here are some specific references for today's tutorial:\n","\n","Requests:\n","* https://docs.python-requests.org/en/master/\n","* https://www.w3schools.com/python/module_requests.asp\n","* https://www.w3schools.com/python/ref_requests_response.asp\n","* https://realpython.com/python-requests/\n","\n","BeautifulSoup:\n","* https://beautiful-soup-4.readthedocs.io/\n","* https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/ (includes good general introduction to webscraping with Python)\n","* https://programminghistorian.org/en/lessons/intro-to-beautiful-soup\n","\n","\n","\n","If you get stuck or don't remember how to do something, it is usually a good idea to **Google** your problem or to ask a **chatbot**. However, although chatbots are very helpful, you must be able to critically assess the quality of their responses. Python has a large (and fast-growing) community and you will probably find answers to most of your questions online (e.g. on **Stack Overflow** or in a **Youtube tutorial**)."]},{"cell_type":"markdown","metadata":{"id":"fMWyFklXzc_k"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"oqPnQTBQ5Uuz"},"source":["### What is web scraping?"]},{"cell_type":"markdown","metadata":{"id":"HelQCK-d0M5c"},"source":["Imagine you have a list of domain names of e.g. companies or organisations and would like to download a certain number of pages from each domain to analyze the content of websites. You may even want to do this every month to monitor the websites. Similarly, you could be interested in tracking data from an online source over time. For example, you may want to collect data on the weather, current health conditions or stock markets.  \n","\n","Of course, you could try to manually call all the relevant pages and copy the data to a file on your computer. However, this will be very time-consuming and error-prone and may simply not be feasible for vast amounts of pages.\n","\n","Web scraping allows you to **retrieve the contents of web pages automatically**. This can be extremely useful if you need data from many different (sub-)pages or if you need to repeatedly download a page over time. Instead of downloading all the pages manually, you can write a program that does all that work for you. Python has several very useful libraries that allow you to do this and is thus a good choice for a web scraping project.\n","\n","Before we get started with these libraries, there are a few things about the internet/web technologies you should know. We will briefly walk you through (1) the basics of data transmission over the internet, (2) the components of a web page and (3) the HTML language."]},{"cell_type":"markdown","metadata":{"id":"uCAtyW3D5bqN"},"source":["### What happens if you type a URL into your browser?"]},{"cell_type":"markdown","metadata":{"id":"vdpGBQuPZq9k"},"source":["When you open your browser and type a URL (e.g. www.google.com) you will usually get directed to a page within milliseconds. Have you ever wondered how this works? Well, the whole process is incredibly complicated – but we can try to provide a simplified answer:\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DudKq-Gmz0SS"},"source":["You can think of the internet as a network that connects two types of computers: **clients** and **servers**. The clients are the ones requesting the information. For example, your computer or your smartphone are clients. Servers are where the information is stored. They are also computers (e.g. a single computer or a whole data center), but they are set up to store and deliver data for the clients.\n","\n","![Client Server Model](https://upload.wikimedia.org/wikipedia/commons/c/c9/Client-server-model.svg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JGDhYxESzaUE"},"source":["When you type ``www.google.ch`` into your browser, your computer (i.e. a client) is requesting information from the Google server. But how does it know where to find this server? Each server (and also each client) is identified through a unique number, the so-called **IP (Internet Protocol) address**. Assume the IP address for the Google server is ``8.8.8.8``.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q-2snw6uzjcT"},"source":["\n","How does your computer figure out that it needs to connect with the server ``8.8.8.8`` when you type ``www.google.com``? This is done through the so-called **Domain Name System (DNS)**. You can think of it as the telephone book of the internet. It consists of servers that store information on all *domain names* (e.g. google.com) and their corresponding *IP addresses*. Your computer will first have to request the IP address from one of these servers (unless it already knows the IP address from previous requests).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o0qaCX0kznja"},"source":["Once your computer has figured out the correct IP address, it will send a so-called **HTTP (Hypertext Transfer Protocol) request** to the corresponding server. HTTP is a protocol that defines how clients and servers have to communicate with each other (i.e. how requests and responses should be written). You can think of an HTTP request as a set of simple instructions stating what the clients wants the server to do. A HTTP request for the Google start page could look as follows:\n","\n","\n","\n","```http\n","GET / HTTP/2\n","Host: www.google.com\n","User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0\n","Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\n","Accept-Language: de,en-US;q=0.7,en;q=0.3\n","Accept-Encoding: gzip, deflate, br\n","Connection: keep-alive\n","Upgrade-Insecure-Requests: 1\n","Cache-Control: max-age=0\n","TE: Trailers\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rg08nw34zqvr"},"source":["\n","The HTTP request will be passed through several layers of protocols (see https://en.wikipedia.org/wiki/OSI_model for further details) and eventually be transformed into signals that can be sent through the telecommunication infrastructure that spans the globe (e.g. fiber cables, satellites etc.). At the destination, the signals will be converted back into a HTTP request. If everything worked well, the server will then produce a **HTTP response** containing the contents (i.e. the source code) of the page the client requested and send it back to your computer (otherwise, it will send back an error message). Your browser will know how to interpret this response and convert it into an appropriately styled web page – for example the Google start page!"]},{"cell_type":"markdown","metadata":{"id":"z_9AXXLv5gkN"},"source":["### What are the components of a website?"]},{"cell_type":"markdown","metadata":{"id":"uq5Fyvi34o43"},"source":["We said that when we request a page, the responsible server will send us a HTTP response containing the source code of the page. But how does this code look like? What are web pages made of?\n","\n","Websites are usually a combination of documents in three languages: HTML, CSS and JavaScript. Let's take a brief look at what each of them is responsible for:\n","\n","* **HTML** – *HyperText Markup Language*: Structured contents of the page (e.g. What are the headings of the page? What do the different paragraphs say? What images do we have?). Additional information at https://developer.mozilla.org/en-US/docs/Web/HTML\n","* **CSS** – *Cascading Style Sheets*: Styling of the page (e.g. What font size and type should headings have? How should the paragraphs be styled?). Additional information at https://developer.mozilla.org/en-US/docs/Web/CSS\n","* **JS** – *JavaScript*: Interactivity of the page (e.g. Show or hide more information with the click of a button; Slide through a carousel of images). Additional information at https://developer.mozilla.org/en-US/docs/Web/JavaScript\n","\n","When we scrape a page, we are usually only interested in its content and will thus only work with the HTML part of a page. So for most tasks, you do not need to worry too much about CSS and Javascript and and can focus on understanding HTML.\n","\n","><font color = 4e1585> SIDENOTE: Note that some understanding of CSS can be helpful in extracting detailed information from a page (see section on CSS selectors)."]},{"cell_type":"markdown","metadata":{"id":"-e8yhn3NzWrh"},"source":["### How does HTML work?"]},{"cell_type":"markdown","metadata":{"id":"aDQL1vFAzand"},"source":["When you scrape a page, you will usually fetch the HTML code of that page and parse it (e.g. with Python's ``BeautifulSoup`` library). To be able to make sense of the code you will have to work with, it is useful to know a few things about HTML. Let's take a brief look at it.\n","\n","HTML is a **markup language**, meaning that it uses **tags** to define **elements** within a document. Let's look at an example:"]},{"cell_type":"markdown","metadata":{"id":"vhsEq8Ic6mRh"},"source":["\n","\n","```html\n","<!DOCTYPE html>\n","<html>\n","<head>\n","<title>Page Title</title>\n","</head>\n","<body>\n","\n","<h1>This is a heading</h1>\n","<p>This is a paragraph.</p>\n","\n","</body>\n","</html>\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aYvNdDmx7O7O"},"source":["Each element **starts with ``<*name of tag*>`` and ends with ``</*name of tag*>``**. For example, the ``body`` element starts with ``<body>`` and ends ``</body>`` and everything in between belongs to this element.\n","\n","Elements are often **nested within each other** (i.e. they contain other elements). For example, the ``h1`` and the ``p`` element are nested within the ``body`` element.  This means that ``body`` is a **parent** of ``h1`` and ``p``. Conversely, ``h1`` and ``p`` are **children** of ``body`` (and **siblings** to each other). Understanding the nested structure of HTML element will be important for extracting specific information from a web page!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c7-cMsS3J2pI"},"source":["There are many different HTML tags (more than 100). You do not need to know all of them, but it is useful to take a look at the most common ones you may encounter (see here for more detailed information on them: https://www.elated.com/first-10-html-tags/):\n","\n","* ``<html>``: Opening tag of every HTML document\n","* ``<head>``: Document head\n","* ``<title>``: Page title (child of ``head`` tag)\n","* ``<body>``: Page content (parent of the main content of the page)\n","* ``<h1>``:  Section heading (``<h2>``, ``<h3>`` etc. for lower level headings)\n","* ``<p>``:  Paragraph\n","* ``<a>``: Link\n","* ``<div>``: Block-level container for content\n","* ``<table>``: Table (with ``<tr>`` tags for each row, a ``<td>`` for each cell and a ``<th>`` tag for table headings)\n","* ``<img>``: Image\n","\n","\n","See https://developer.mozilla.org/en-US/docs/Web/HTML/Element for a comprehensive list of HTML elements"]},{"cell_type":"markdown","metadata":{"id":"3GHprYDZy5Gd"},"source":["Tags can also have **attributes**. Consider the following example:\n","\n","* ``<a href=\"www.someaddress.com\" target=\"_blank\"> Click on this link</a>``\n","\n","The ``<a>`` tag above has two attributes. The ``href`` attribute defines the URL and the ``target`` attribute specifies how it should be opened (`\"_blank\"` means that it will be opened in a new tab). Each tag has its own set of attributes, but there are also global attributes such as `id` or `class` that can be used with any tag. See the following links, if you would like to know more:\n","* https://www.w3schools.com/html/html_attributes.asp\n","* https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N3OxwZmG4tex"},"source":["\n","\n","---\n","### **<font color='teal'>In-class exercise**\n","\n",">  <font color='teal'> Check out this simple page to see how all of this works: http://farys.org/daten/example-page.html. If you right-click on the page, you should be able to select \"View source code\". You can also click on \"Inspect\" (German: \"Element untersuchen\") to see which part of the code refers to which part on the web page! If you need to scrape a particular element from a page, this is usually a good starting point.\n",">\n",">  <font color='teal'> Try to answer the following questions:\n",">\n",">*  Can you find the element with the main heading?\n","* What are the parent, children and siblings of the ``table`` element?\n","* In which element is the \"Bongo Cat\" link? What attributes does the tag have?\n","* Extra (after-class) task: Go to the Bongo Cat page and learn to play \"Happy Birthday\" with the Marimba! ;-)\n","\n","\n","\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MXtn839BzxMq"},"source":["## Scraping web pages with ``requests``"]},{"cell_type":"markdown","metadata":{"id":"6F-esFUJNT0f"},"source":["### Making simple requests"]},{"cell_type":"markdown","metadata":{"id":"6qSFxYy10jAo"},"source":["So let's get started with web scraping. How can we use Python to **retrieve a page from the internet**? There are several libraries that allow you to do this. The most popular one is the **``requests`` library** (https://docs.python-requests.org/en/master/). We can import it as follows:"]},{"cell_type":"code","metadata":{"id":"AK5c4NzSCfkj"},"source":["import requests"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGMopdr9CiSB"},"source":["Now let's request the cat page from the example above using the **``get`` function**. It will send an HTTP request to the responsible server and fetch the response for you:"]},{"cell_type":"code","metadata":{"id":"sb6I2iAlCmWq"},"source":["r = requests.get(\"http://farys.org/daten/example-page.html\")\n","type(r)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wgI88KJFuYM"},"source":["As you can see, we got some kind of response object. What can we do with it? A first useful thing to do is to **check the status code** (by calling the ``status_code`` attribute):"]},{"cell_type":"code","metadata":{"id":"bdfYLTxpGaN4"},"source":["r.status_code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzU04dx8Gozf"},"source":["What does this mean? Simply put, the 2xx (i.e. 200, 201 etc.) are successes, the 3xx codes are redirects, the 4xx codes are client errors and the 5xx codes are server errors (see here for a list of all HTTP status codes: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes or https://developer.mozilla.org/en-US/docs/Web/HTTP/Status). So in our case, everything seems to have gone well so far as we got a status code of 200."]},{"cell_type":"markdown","metadata":{"id":"RizupGcdGK1c"},"source":["Usually, we are interested in the source code of the page. We can access it through the **``text`` attribute**:"]},{"cell_type":"code","metadata":{"id":"zoNGA2WyFtHM"},"source":["r.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYdhkNvwTeQC"},"source":["type(r.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a5H9V0ngJdG9"},"source":["As you can see, we now have a string with the HTML code of the page – it is the same code you saw when clicking on \"View source code\" on the page!"]},{"cell_type":"markdown","metadata":{"id":"AC0IV5rBNJn-"},"source":["###Setting query parameters*"]},{"cell_type":"markdown","metadata":{"id":"A2xkUtWiJ8Wu"},"source":["<font color='gray'>This section is for self-study and will not be discussed in class.\n","\n","You will often encounter URLs that look somewhat like this:\n","\n","* https://en.wikipedia.org/w/index.php?title=Cat&action=info (information page about the \"Cat\" page on Wikipedia)\n","\n","The part after the ``?`` is called a **query string** (https://en.wikipedia.org/wiki/Query_string; For a general overview on the structure of URLs see https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL). It is a way to send additional information to the server, for example search terms when you use Google. The server will still deliver an HTML page, but the information in the query string might be used to alter this page before it is sent to you.\n","\n","Instead of passing in the entire URL to the ``get`` function of the `requests` library, you can also provide the parameters for the query string as a separate argument. This is done by passing a dictionary as the argument of the **``params`` parameter**:"]},{"cell_type":"code","metadata":{"id":"8er1aVyoODqn"},"source":["# Same result as:\n","# cat = requests.get(\"https://en.wikipedia.org/w/index.php?title=Cat&action=info\")\n","\n","my_params = {\"title\" : \"Cat\",\n","             \"action\" : \"info\"}\n","cat = requests.get(\"https://en.wikipedia.org/w/index.php\",\n","                   params=my_params)\n","\n","print(cat)\n","print(cat.url) # Look at url that was used for the request"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33LZu0eXPsUv"},"source":["Why would this be useful? Suppose you would like to scrape not only the \"Cat\" info page, but also the ones for many other animals. Of course, you could write a loop that builds the different URL strings for you. But it is much cleaner and easier to use the ``params`` parameter:"]},{"cell_type":"code","metadata":{"id":"XZWsDNZBQV77"},"source":["animals = [\"Cat\", \"Dog\", \"Bird\"]\n","\n","L=[]\n","for animal in animals:\n","  my_params = {\"title\" : animal, \"action\" : \"info\"}\n","  res = requests.get(\"https://en.wikipedia.org/w/index.php\", params=my_params)\n","  L.append(res)\n","\n","L # List with response objets for the three animal info pages"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mUHoVX2Mbtq"},"source":["><font color = 4e1585> SIDENOTE: The ``get`` function also allows you to specify other useful arguments. See here for an introduction: https://docs.python-requests.org/en/master/user/quickstart/."]},{"cell_type":"markdown","metadata":{"id":"h7ohZzY1HJfT"},"source":["\n","\n","---\n","###**<font color='teal'>In-class exercise**\n","\n",">  <font color='teal'> Retrieve the Wikipedia page about cats (https://en.wikipedia.org/wiki/Cat) and assign the response to a variable named ``resp``. Print the status code and the text of the response.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Pnz3aCwpKRde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"N1sP25uvLOKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g2Ff2QdbLOXK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHs05S6RUV5F"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"xwVtDm70z2OH"},"source":["## Extracting data from HTML with ``BeautifulSoup``"]},{"cell_type":"markdown","metadata":{"id":"efUk4LVtTyMP"},"source":["We now have a string with the HTML code of the page. Usually we are not interested in the entire code, but would like to **extract specific information** from it. For example, we might be interested in the headings, the links, some paragraph, a table etc. How can we access these things?\n","If you wanted to do this all by yourself, you would have to write a lot of complicated code searching for text patterns in the HTML string. Luckily, extracting information from HTML code (or other markup languages) is much more convenient thanks to libraries such as ``BeautifulSoup``."]},{"cell_type":"markdown","metadata":{"id":"HT0UqEWUyx6j"},"source":["\n","The ``BeautifulSoup`` library allows you to **parse data from HTML** (and XML) files. This means that you can convert the HTML string into an object that will be easier to search. Moreover, it provides you with many useful **functions and methods to locate the information** you are interested in. We will only be able to cover a few of these functions and methods. Check out the documentation if you want to learn more:\n","https://beautiful-soup-4.readthedocs.io/"]},{"cell_type":"markdown","metadata":{"id":"_-BhXcCAl2Wt"},"source":["### Parsing HTML strings"]},{"cell_type":"markdown","metadata":{"id":"bdMzRyTHfdRk"},"source":["Let's get started and import the ``BeautifulSoup`` library:"]},{"cell_type":"code","metadata":{"id":"cZR3PeNsdRAZ"},"source":["from bs4 import BeautifulSoup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TYLEGXwGfz3x"},"source":["We will continue to work with the HTML string from the cat example page:"]},{"cell_type":"code","metadata":{"id":"hCyXPJCugDtu"},"source":["r = requests.get(\"http://farys.org/daten/example-page.html\")\n","r.text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfxTYiXQVWvB"},"source":["type(r.text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVIfiX8agg4I"},"source":["First, we need to parse it to convert it into a ``BeautifulSoup`` object:"]},{"cell_type":"code","metadata":{"id":"L_E-1ssDgqJX"},"source":["soup = BeautifulSoup(r.text)  # creates an object of the BeautifulSoup class\n","type(soup)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oscs22IYiKfX"},"source":["It looks as follows:"]},{"cell_type":"code","metadata":{"id":"eQtArBxmiLNl"},"source":["soup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"diYtBGcSjWex"},"source":["Another way to print out the content of the object is using the ``prettify()`` method. It allows you to print the HTML code in a way that makes it easier to read; indentation is use to show how the tags are nested within each other:"]},{"cell_type":"code","metadata":{"id":"VrOhRLf8hTaj"},"source":["print(soup.prettify())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xkzstx3Iln14"},"source":["### Accessing elements with ``find`` and ``find_all``"]},{"cell_type":"markdown","metadata":{"id":"WGzoaUSPhSmN"},"source":["Now, how could we **access different elements** within this \"soup\"? Suppose, we would like to access the ``head`` element. You can do this using the **``find``** method:"]},{"cell_type":"code","metadata":{"id":"r-GAudRRj4TM"},"source":["soup.find(\"head\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYSJDdhf9oZ4"},"source":["Similarly, we could access the ``title`` element:"]},{"cell_type":"code","metadata":{"id":"ziJFFosG9uF3"},"source":["soup.find(\"title\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Egol9ODq7Ktj"},"source":["As the ``title`` and the ``head`` elements are stored as an **attribute** of the ``soup`` object, we can also access them like this:"]},{"cell_type":"code","metadata":{"id":"NJGDesLS-U48"},"source":["soup.head"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ng1Qs24H7YoT"},"source":["soup.title"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7kgv-F978YR"},"source":["The return object will be a *BeautifulSoup tag*:"]},{"cell_type":"code","metadata":{"id":"UPQTINzh8e3o"},"source":["type(soup.head)  # Or: soup.find(\"head\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emfBYdQx8meg"},"source":["This means that we can continue to search it. Let's try to get the ``title`` element within the ``head`` element:"]},{"cell_type":"code","metadata":{"id":"jEITlZx_8xyx"},"source":["soup.head.title # Or: soup.find(\"head\").find(\"title\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbT5CI8R9MzQ"},"source":["*(Note that in this case, just typing ``soup.title`` returns the same result. However, this will not always be the case as we are now explicitly searching for the title element **within** the head element.)*"]},{"cell_type":"markdown","metadata":{"id":"MkVQ2aMpkESu"},"source":["Let's now try to access a link. Links are within ``a`` tags:"]},{"cell_type":"code","metadata":{"id":"Mt9izO8gk8Z0"},"source":["soup.find(\"a\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JX8GOrL2laXC"},"source":["But wait, there are several links on the website! Why did we get only one? The **``find`` method always returns the first element** that matches the search criteria. If we would like to get **all the matching elements, we can use the ``find_all`` method**:"]},{"cell_type":"code","metadata":{"id":"OSNpCnzAmjRt"},"source":["soup.find_all(\"a\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW1ffIUlieI5"},"source":["Now all instances of the `a` tag are returned in a list."]},{"cell_type":"markdown","metadata":{"id":"uTCleoy-sVYH"},"source":["><font color = 4e1585> SIDENOTE: Because ``find_all`` is such a commonly used method, there is a shortcut for it. For example, instead of writing ``soup.find_all(\"a\")``, you could also just write ``soup(\"a\")``."]},{"cell_type":"markdown","metadata":{"id":"IPS1X88roLN_"},"source":["We can also **refine our search by specifying arguments**. For example, we can use the **attrs** parameter to search tags based on HTML attributes (see section on HTML). Let's fetch the table with the id \"famous_cats_table\" and the class \"cat_table\":"]},{"cell_type":"code","metadata":{"id":"8c3fWyyQov8J"},"source":["soup.find_all(\"table\", attrs={\"id\": \"famous_cats_table\", \"class\": \"cat_table\"})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyGS7VbpA13I"},"source":["Most HTML attributes can also be addressed directly by using the name of the attribute as the argument key in ``find_all()``:\n"]},{"cell_type":"code","metadata":{"id":"rG79Eo138Sxu"},"source":["soup.find_all(\"table\", id=\"famous_cats_table\", class_=\"cat_table\")\n","# Note that you need to write class_, not class (class is a reserved keyword)!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_nrgH223lPnQ"},"source":["### Extracting text and attribute values of an element"]},{"cell_type":"markdown","metadata":{"id":"rbjmTdEHj9Y1"},"source":["So far, so good – but the outputs we got so far are not completely satisfactory. They still have a lot of HTML markup around them that we do not want. How can we get rid of it? We can use the **``get_text`` method** to get only the text of a tag:"]},{"cell_type":"code","metadata":{"id":"bAfIQEkBp7by"},"source":["soup.find(\"title\").get_text()  # or using attribute notation: soup.title.text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rV4-W7DkqYfv"},"source":["Let's try the same for the first URL:"]},{"cell_type":"code","metadata":{"id":"x_yQFF7IqkY3"},"source":["link = soup.find(\"a\")\n","print(link)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zMmQvsMsqOCA"},"source":["link.get_text()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YbzjhS3qeD4"},"source":["That's not quite what we wanted. How can we access the URL?\n","\n","As you may have noticed, the URL is part of the **HTML attributes of the ``a`` tag** (see section on HTML). In ``BeautifulSoup``, these attributes can be **treated like a dictionary**. In our case, we have two key:value pairs: the ``href`` key with a value of ``\"https://bongo.cat/\"`` and the ``target`` key with a value of ``\"_blank\"``. To get the URL, you can thus simply type:"]},{"cell_type":"code","metadata":{"id":"cKoczi0gqy4N"},"source":["link[\"href\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qb1N-AcVEdrH"},"source":["So what if we wanted to get all the URLs in the document. Let's try:"]},{"cell_type":"code","metadata":{"id":"RaRL09nVEsId"},"source":["links = soup.find_all(\"a\")\n","type(links)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q5zGjJrhCOt"},"source":["links"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WW3pW0vIFBB7"},"source":["# links[\"href\"] # this will return an error!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fh2y8644FI3L"},"source":["Why did't this work? Our ``links`` variable does not contain a single element, but a **BeautifulSoup ``ResultSet`` containing several elements**. The set can be treated like a list. For example, you could extract the link for the first element in the ``ResultSet`` like this:"]},{"cell_type":"code","metadata":{"id":"TSp1wmACHdY5"},"source":["links[0][\"href\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L76aFsvBHchp"},"source":["If you want to extract the link for all the elements, you can write a **loop or a list comprehension**:"]},{"cell_type":"code","metadata":{"id":"vYDYBjuyFz7W"},"source":["# Loop to print all the URLs\n","for tag in links:\n","  print(tag[\"href\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["L = []\n","for tag in links:\n","  L.append(tag[\"href\"])\n","L"],"metadata":{"id":"Zes2tl7MRVFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNiC1X_UGGQX"},"source":["# Better: List comprehension to write all the URLs into a list\n","L = [tag[\"href\"] for tag in links]\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2MySC5fIDmJ"},"source":["### Accessing elements with ``select``/CSS selectors*"]},{"cell_type":"markdown","metadata":{"id":"1VE7pSSKO0Ms"},"source":["<font color='gray'>This section is for self-study and will not be discussed in class.\n","\n","The ``BeautifulSoup`` library also features the ``select()`` method that allows you to select elements in a more sophisticated way: through so-called **CSS selectors**. CSS selectors are patterns that are used in CSS to select the elements (within the HTML file) you want to style (see here for an overview: https://www.w3schools.com/cssref/css_selectors.asp). You can use these selectors to extract elements through BeautifulSoup's ``select`` method.\n","\n","The ``select`` method may be a bit harder to learn, but it is very flexible and powerful. Let's look at some examples so you get a feeling of what is possible:"]},{"cell_type":"code","metadata":{"id":"iBHyURM0R7bV"},"source":["soup.select(\"body h2\") # Select all h2 tags within the body tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vd5KZoy0SCLD"},"source":["soup.select(\"body table td\") # Select all td tags within a table tag within a body tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxTSvgdPSg24"},"source":["soup.select(\"tbody > tr\") # Select all tr tags where the parent is a tbody tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzlZSUGsUzlN"},"source":["soup.select(\"h2 ~ p\") # Select all  p tags that are preceded by a h2 tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsNMpEvNSNkQ"},"source":["soup.select(\"#famous_cats_table\") # Select the tag with the id \"famous_cats_table\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmMHP2IWTLFE"},"source":["soup.select(\".cat_table\") # Select the tag with the class \"cat_table\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"begb8nsFWPh6"},"source":["For the documentation on the ``select`` method, see here: https://beautiful-soup-4.readthedocs.io/en/latest/index.html#css-selectors"]},{"cell_type":"markdown","metadata":{"id":"FrnCHAGJK1t-"},"source":["To know how to specify the selection criteria, it can be useful to first explore the nested structure of the code. There are many useful functions that allow you to **navigate through the \"family tree\" of HTML elements**."]},{"cell_type":"code","metadata":{"id":"gUu4vexYLWYy"},"source":["list(soup.head.children) # Get all children of the head tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ienlgiQIIivY"},"source":["soup.title.parent # Get the (direct) parent of the title tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5FziJZ1LuJq"},"source":["list(soup.title.next_siblings) # Get all siblings that come after the title tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQQIlKBPLivl"},"source":["# and many more"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2mB0tC5WLfU"},"source":["---\n","###**<font color='teal'>In-class exercise**\n",">  <font color='teal'>\n","We will continue to work with the response object from the example page:"]},{"cell_type":"code","metadata":{"id":"Y3YNthf2NXqI"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Fasd-znZc37"},"source":[">  <font color='teal'> Convert the response into a BeautifulSoup object called ``cats_soup``."]},{"cell_type":"code","metadata":{"id":"N60UmfWyZual"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYYARh1SZpjK"},"source":[">  <font color='teal'> Now try to extract the following:\n","* The ``h1`` element\n","* The second ``h2`` element\n","* The text of the ``h1`` element\n","* All elements of the ``class`` \"cat_table\""]},{"cell_type":"code","metadata":{"id":"atZlOWd3cO8R"},"source":["# h1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChnBGn43cOZU"},"source":["# second h2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZsuDAGWcOzK"},"source":["# text of h1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4s4XPULcOoU"},"source":["# all cat tables\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WUEJZTQ4WQ3N"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-fUey_e7uoH1"},"source":["## Extracting HTML tables with Pandas*"]},{"cell_type":"markdown","metadata":{"id":"ovF4uyWeZNsg"},"source":["<font color='gray'>This section is for self-study and will not be discussed in class.\n","\n","Now suppose you would like to extract a table from a webpage and write it to a Pandas dataframe so that you can continue to work with it. The table element of the example page looks like this:"]},{"cell_type":"code","metadata":{"id":"hf3uYv5fY_lA"},"source":["soup.find(\"table\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uK2aZwnjYxWi"},"source":["Of course, you could continue to use BeautifulSoup and write a complicated loop to convert this HTML code into a Pandas dataframe.\n","\n","Luckily, there is an easier solution: Pandas provides the read_html function that does this automatically for you! You only need to provide the URL and it will fetch you all the tables and return them as a list of Pandas dataframes (https://pandas.pydata.org/docs/reference/api/pandas.read_html.html):"]},{"cell_type":"code","metadata":{"id":"IEejHOxbcibN"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fpl4mOkgYwsD"},"source":["tables_list = pd.read_html(\"http://farys.org/daten/example-page.html\") # request tables from example page\n","type(tables_list) # return object is a list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHTSx66aeKco"},"source":["len(tables_list) # one list element for each table on the page"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EmHTBfkdIjU"},"source":["As there is only one table on our example page, our return object will be a list with one element. Let's fetch the first (and only) element in this list:"]},{"cell_type":"code","metadata":{"id":"bEiuhqLPdg2R"},"source":["cat_table = tables_list[0] # access first element\n","cat_table"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4i5pkLFds06"},"source":["type(cat_table) # Pandas dataframe representing the table from the website"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sn2qPHZEfJF3"},"source":["><font color = 4e1585> SIDENOTE: As you have seen, the read_html can fetch the tables from a website and return them as Pandas dataframes. Moreover, it can also be directly applied to a HTML string."]},{"cell_type":"markdown","metadata":{"id":"IRgZ9Cn7w-GW"},"source":["## Web scraping in practice"]},{"cell_type":"markdown","metadata":{"id":"7zIWvxY4se5r"},"source":["### Scraping many pages at once"]},{"cell_type":"markdown","metadata":{"id":"GaLgPZPUxDUW"},"source":["Retrieving information from a single page through web scraping is often not very useful – you might as well just go to the page and copy the content you need. The power of web scraping starts when we **retrieve many pages automatically**. How could we do this?\n","\n","The answer to this question depends on your project:\n","\n","1. In some cases, you have a pre-defined list of URLs you would like to scrape. You could then loop through all of them to fetch all the pages and retrieve the contents you are interested in.\n","\n","2. If you are scraping many pages of the same domain you can try to figure out the structure of the different URLs you want to fetch. You can then write a loop to generate all the strings of these URLs and fetch the respective pages. An example would be a website where pages are available for every day, e.g.: example.org/data/2000-01-01, ... ,example.org/data/2023-08-24. In this example you would need to figure out the earliest and latest dates.\n","\n","3. In some cases, you can start on one page, retrieve the URLs you find on this page, then request the respective pages and retrieve the URLs on them ... and so on. This is called **web crawling** *(think of the example in the introduction where we started with a domain name and wanted to retrieve a certain number of pages)*.\n","\n","Let's take the list of URLs we had on our cat example page to build a very simple web scraper:\n","\n"]},{"cell_type":"code","metadata":{"id":"2EzDLQ-n7FNu"},"source":["# Get list of links to cat pages\n","wiki_links = soup.table.find_all(\"a\")\n","wiki_links = [link[\"href\"] for link in wiki_links]\n","wiki_links"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhUqJ1llZAII"},"source":["Suppose you would like to fetch the page title and the number of outgoing links from each of these pages. Let us write a loop that does this:"]},{"cell_type":"code","metadata":{"id":"3fSDc6v0ZVow"},"source":["# Write simple web scraper\n","L = []\n","\n","for link in wiki_links:\n","  resp = requests.get(link).text\n","  s = BeautifulSoup(resp)\n","\n","  title = s.title.get_text()\n","  nr_links = len(s.find_all(\"a\"))\n","\n","  L.append([title, nr_links])\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGiPCklAYRGL"},"source":["With this, we have already built a simple web scraper!\n","\n","When you work on larger web scraping projects, **things will often not go so smoothly**. For example, you might run into different types of server (or client) errors when requesting a page. Moreover, different pages are often structured differently and the tags you want to extract may not exist (or contain something else) on some pages. To make your web scraper more robust, you may have to **define conditions or exceptions** to make sure that it can handle all the problems that might occur.\n"]},{"cell_type":"markdown","metadata":{"id":"tqCzyFKChOE5"},"source":["\n","Moreover, it is often a good idea to **divide your project into two separate tasks**:\n","\n","1. *Data collection*: Fetch the pages and save the data fo a file (or several files) on your computer.\n","2. *Information extraction*: Read the file(s) and extract the information you need from it\n","\n","Let's adapt our code to follow this paradigma:"]},{"cell_type":"code","metadata":{"id":"WOhedDHdjzPW"},"source":["# Retrieve URLs and save them to your computer/drive\n","for i, link in enumerate(wiki_links):\n","  resp = requests.get(link).text\n","  filename = f\"wiki_cat_file_{i}.html\"\n","\n","  with open(filename, \"w\") as file:  # use the open() function to import/export files\n","    file.write(resp)\n","\n","# Load data from the files and retrieve elements\n","L = []\n","for i in range(len(wiki_links)):\n","  filename = f\"wiki_cat_file_{i}.html\"\n","  s = BeautifulSoup(open(filename))\n","\n","  title = s.title.get_text()\n","  nr_links = len(s.find_all(\"a\"))\n","\n","  L.append([title, nr_links])\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IAMz4uB2psg5"},"source":["For a simple example like ours, this does not make much sense and only makes things unnessarily complicated. But for larger projects where you may run into many kinds of problems and need to handle large amounts of data, this may actually make things easier!\n","\n","If you feel like going full nerd on a rainy Sunday, check out these two talks about web scraping projects:\n","* Spiegel Online Mining: https://www.youtube.com/watch?v=-YpwsdRKt8Q\n","* Deutsche Bahn Mining: https://www.youtube.com/watch?v=0rb9CfOvojk\n","\n","English versions:\n","* https://www.youtube.com/watch?v=bYviBstTUwo\n","* https://www.youtube.com/watch?v=AGCmPLWZKd8\n","\n","They are fun to watch but they also show you very typical webscraping pitfalls and solutions (scaling issues, umlauts, legal issues, ..)"]},{"cell_type":"markdown","metadata":{"id":"Us0Eea5XsALh"},"source":["### Be nice!"]},{"cell_type":"markdown","metadata":{"id":"4VShn8Qqs5lL"},"source":["Webscraping allows you to request large numbers of pages within seconds. This can **impose considerable load on the servers** – for them it is as if many people used the pages in their browsers. If things go badly, you can make a server slow down considerably (or even crash).\n","\n","Many 'free' resources on the web (e.g. Wikipedia) are shared by all of us and their infrastructure needs to be payed for somehow (e.g. by donations). So limit your requests to a polite rate and don't scrape more information than you need to!\n","\n","Some websites protect themselves against webscraping/webcrawling by imposing **rate limits**. For example, you may only be allowed to call a page every 2 seconds. When you get rate limited, you will mostly get a status code of ``429`` and not be able to retrieve the page.\n","\n","You can **make your program scrape more slowly** using the **``sleep``  function** of the ``time`` module. This function works as follows:"]},{"cell_type":"code","metadata":{"id":"KCIllLLawrSd"},"source":["import time\n","print(\"Printed immediately.\")\n","time.sleep(2)\n","print(\"Printed after 2 seconds.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7S5h8sftw-j5"},"source":["Let's make our Wikipedia scraper retrieve the pages more slowly:"]},{"cell_type":"code","metadata":{"id":"2XGltCUIxcvl"},"source":["L = []\n","\n","for link in wiki_links:\n","  time.sleep(1) # sleep for one second\n","  resp = requests.get(link).text\n","  s = BeautifulSoup(resp)\n","\n","  title = s.title.get_text()\n","  nr_links = len(s.find_all(\"a\"))\n","\n","  L.append([title, nr_links])\n","\n","L"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPtZD15uxoVr"},"source":["Finally, there are some websites that disallow web scraping. Many websites have a ``robot.txt`` file that specifies if there are things you should not do. For example, this is the ``robot.txt`` file for the Wikipedia page: https://en.wikipedia.org/robots.txt.\n","\n","For further details see:\n","* https://en.wikipedia.org/wiki/Robots_exclusion_standard\n","* https://developers.google.com/search/docs/advanced/robots/robots_txt"]},{"cell_type":"markdown","metadata":{"id":"Me7-e0mVAuto"},"source":["### Other tools and data sources"]},{"cell_type":"markdown","metadata":{"id":"nnpPZcbhUv09"},"source":["Appart from ``BeautifulSoup`` there are two other popular web scraping libraries for Python:\n","\n","* ``Scrapy`` (https://scrapy.org)\n","* ``Selenium`` (https://selenium-python.readthedocs.io)\n","* ``Playwright`` (https://playwright.dev)\n","\n","These will take more time to learn, but depending on your project, it will be worth the effort. Watch this video for a short comparison: https://youtu.be/zucvHSQsKHA"]},{"cell_type":"markdown","metadata":{"id":"fcqYaASW6aTx"},"source":["In this tutorial we worked with 'live' versions of webpages. If you are interested in scraping pages over time (especially for the past), you could work with so-called web archives. The following are two well-known archives:\n","\n","* http://commoncrawl.org\n","* https://archive.org\n","\n","Note, however, that accessing the web archives can be rather involved and you may need some time to find out how you can retrieve the archived pages you are interested in."]}]}