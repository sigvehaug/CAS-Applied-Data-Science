{"cells":[{"cell_type":"markdown","id":"c6378169","metadata":{"id":"c6378169"},"source":["# Web Scraping II\n","#### CAS Applied Data Science 2025 ####\n","\n","\n","In the previous tutorials, we learned how to automatically retrieve pages from the internet through **web scraping**. We sent HTTP requests with the  ``requests`` library and used the ``BeautifulSoup`` library to parse and work with the HTML code from the response we got. This is a good start and works well for so-called **static** pages, but when scraping **dynamic** websites that use JavaScript in the background, you will notice that this approach often fails. Today, we will learn how to deal with this problem."]},{"cell_type":"markdown","id":"dd74906f","metadata":{"id":"dd74906f"},"source":["## Scraping dynamic websites\n","\n","Imagine you want to scrape all currently listed open positions of \"data scientists\" in \"Bern\" on www.indeed.com.\n","\n","Let's have a look at the output in a browser: https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE\n","\n","We could load the website with ``requests`` and extract the parts that we want using ``BeautifulSoup``:"]},{"cell_type":"code","execution_count":null,"id":"498677f1","metadata":{"scrolled":true,"id":"498677f1"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","res = requests.get(\"https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE&vjk=4d0278f36b754f74\")\n","res.text\n","\n","# information sits in <li> elements under <ul class='jobsearch-ResultsList'>, but there might be other ways to get to it too...\n","soup = BeautifulSoup(res.text)\n","soup\n","soup.find_all(\"p\")"]},{"cell_type":"markdown","id":"bc983825","metadata":{"id":"bc983825"},"source":["That looks bad. The webserver recognises that we are not a regular user entering normally through a browser. What could we do from here?"]},{"cell_type":"code","execution_count":null,"id":"7821a216","metadata":{"scrolled":false,"id":"7821a216"},"outputs":[],"source":["url = 'https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE'\n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n","}\n","\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(res.text)\n","\n","soup.find_all(\"p\")"]},{"cell_type":"markdown","id":"4e2d5f82","metadata":{"id":"4e2d5f82"},"source":["Still not working. What is going on here? If the html source code you retrieve from a page does not correspond to what you see when viewing this page in your browser, you are probably dealing with a **dynamic web page**. Typically, this means that some JavaScript code is executed in the background to \"create\" the source code of the final page you see. When you scrape a dynamic page with the requests library, you will not get the final html code, but often an incomplete version where you might only see the instructions to run the JavaScript files. Some web pages also use dynamic features to actively prevent web scraping. You might then get an error message like the one in the example above. Luckily, there is another Python package we can use to address such problems."]},{"cell_type":"markdown","id":"e323dea8","metadata":{"id":"e323dea8"},"source":["## Selenium\n","\n","``selenium`` is a Java based software that enables you to automate browsers (e.g. Chrome, Firefox etc.). You can think of it as an interface that allows you to control what your browser does through code. It was developed for automated website-testing, but is also very useful for scraping web pages, especially those that require JavaScript rendering. It can automate web browsing and interactions, like clicking buttons or filling out forms, and supports a wide range of browsers, including Firefox, Chrome, and Edge. Selenium can be used in multiple programming languages including Python. However, it might require an installation of browser drivers as well as Java and can sometimes be a bit tedious to set up.\n","\n","Let's start by installing the ``selenium`` Python package.\n","\n","**Note that you cannot use ``selenium`` from within Colab as the Python instance is running in the cloud where no browser is available. Copy the notbook of this tutorial to your local machine and run the code in, e.g., Jupyter Notebook.**"]},{"cell_type":"code","execution_count":null,"id":"52a6f130","metadata":{"id":"52a6f130"},"outputs":[],"source":["# Selenium can be installed via pip\n","!pip install selenium"]},{"cell_type":"markdown","id":"9677efbe","metadata":{"id":"9677efbe"},"source":["Selenium requires a driver (e.g. chromedriver) to communicate with your favorite browser (e.g. chrome). In the newest version of the selenium module, webdrivers for different browsers should be installed automatically. If this is not the case, you can install them manually.\n","\n","**A list of available drivers:**\n","\n","* Chrome:\n"," - https://chromedriver.chromium.org/downloads\n","\n","* Edge:\n"," - https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n","\n","* Firefox:\n"," - https://github.com/mozilla/geckodriver/releases\n","\n","* Safari:\n"," - https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n","\n","If the code below not work, make sure you have an appropriate driver (matching version) installed and that the driver is accessible, i.e. it is located in your ``PATH`` environment. If you want to know where the selenium package was stored on your computer, you can import it and then type ``print(selenium.__file__)``.\n","\n","You can read up setup instructions here: https://pypi.org/project/selenium/\n","\n","Let's try to import the webdriver, initialize it and got to 'https://ch.indeed.com' (i.e. send a get request).  \n"]},{"cell_type":"code","execution_count":null,"id":"11bc1f42","metadata":{"id":"11bc1f42"},"outputs":[],"source":["from selenium import webdriver\n","browser = webdriver.Chrome()\n","browser.get('https://ch.indeed.com')"]},{"cell_type":"markdown","id":"bb65d4d5","metadata":{"id":"bb65d4d5"},"source":["This opens your preferred browser. With selenium you can now take control of what the browser does, e.g.\n","* fill in textfields,\n","* click on elements,\n","* scroll,\n","* etc.\n"]},{"cell_type":"markdown","id":"ee258d7e","metadata":{"id":"ee258d7e"},"source":["><font color = 4e1585> SIDENOTE: If the version of the driver and the version of your browser do not match, you will get an error. One way to address it is by using selenium's ``webdriver_manager``, which allows you to automatically install the driver version that corresponds to your browser (and also bypasses problems regarding the PATH environment):\n",">```python  \n","!pip install webdriver_manager\n","from selenium import webdriver\n","from webdriver_manager.chrome import ChromeDriverManager\n","browser = webdriver.Chrome(ChromeDriverManager().install())\n","```"]},{"cell_type":"markdown","id":"ffca01ab","metadata":{"id":"ffca01ab"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Use Jupyter or an IDE (Spyder/pyCharm). Install ``selenium`` and try to start a selenium controlled browser window (Chrome/Firefox/Edge/Safari). If it does not immediately work, don't panic. Depending on your operating system, python version, selenium version, browser version etc. all kind of problems may occur! If it does not quickly work, take your time and try to find the problem after the course. Also note: we might get blocked from indeed when we do this in the course.\n","<font color='teal'> Make sure that:\n","\n","* <font color='teal'>Both browser and selenium is the current version (e.g. force an update if you have an old version of selenium)\n","* <font color='teal'>and/or update your browser\n","* <font color='teal'>Depending on version and system you might need to manually install the driver for your browser.\n","* <font color='teal'>If the verions of your driver and your brwoser don't match, you can use selenium's built-in functionality to install a driver via the ``webdriver_manager`` (see sidenote above).\n"]},{"cell_type":"code","execution_count":null,"id":"95ccb195","metadata":{"id":"95ccb195"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"b57ae5ec","metadata":{"id":"b57ae5ec"},"source":["\n","---\n"]},{"cell_type":"markdown","id":"36aa733d","metadata":{"id":"36aa733d"},"source":["### Filling in text fields\n","\n","Now that we have a browser window running that is controlled by our Python code, selenium allows us to do many different things (see here for all available methods: https://www.simplilearn.com/tutorials/python-tutorial/selenium-with-python). For example, we could send keystrokes to the searchbar.\n","\n","We want to search for positions for data scientists. First, we need to locate the search fields. This can be done using the ``find_element`` method. It allows us to find matching elements on a website based on locator values such as the tage name, (ByTAG_NAME), id (By.ID), the class (By.Class), css selectors (By.CSS) and much more. If we inspect the source code of the page, we will see that the id of the tag containing the \"What\" field (for the job title) is \"text-input-what\" while the one for the \"Where\" field is 'text-input-where'. Let's start by locating the \"What\" field:"]},{"cell_type":"code","execution_count":null,"id":"bfbbae74","metadata":{"id":"bfbbae74"},"outputs":[],"source":["from selenium.webdriver.common.by import By # To find elements\n","from selenium.webdriver.common.keys import Keys # For special keys (Enter, delete, down etc.)\n","#elem = browser.find_element(By.CSS_SELECTOR, \"[type='checkbox']\")\n","#elem.click()"]},{"cell_type":"code","source":["elem = browser.find_element(By.ID, 'text-input-what') # Find element"],"metadata":{"id":"bwkA6JYhP1S5"},"id":"bwkA6JYhP1S5","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"43864e63","metadata":{"id":"43864e63"},"source":["Now we can send the keytrokes to the selected field:"]},{"cell_type":"code","execution_count":null,"id":"4fde9692","metadata":{"id":"4fde9692"},"outputs":[],"source":["elem.send_keys('data scientist') # enter search query for \"data scientist\""]},{"cell_type":"markdown","id":"ed9ec400","metadata":{"id":"ed9ec400"},"source":["Let's limit our search to Bern and hit Enter to get the job openings:"]},{"cell_type":"code","execution_count":null,"id":"109f8d71","metadata":{"id":"109f8d71"},"outputs":[],"source":["elem = browser.find_element(By.ID, 'text-input-where') # find \"where\" field\n","elem.send_keys('Bern, BE' + Keys.RETURN) # enter \"Bern. BE\" and hit Enter!"]},{"cell_type":"markdown","id":"b78d8c1c","metadata":{"id":"b78d8c1c"},"source":["---\n","\n"," <font color='teal'> **In-class exercise**:\n","Can you click (method ``.click()``) on the drop down menu for the language?"]},{"cell_type":"code","execution_count":null,"id":"df2b46f4","metadata":{"id":"df2b46f4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"95f9b51f","metadata":{"id":"95f9b51f"},"source":["---"]},{"cell_type":"markdown","id":"efbc60fc","metadata":{"id":"efbc60fc"},"source":["### Extracting elements\n","\n","From here we can fetch the pages source code and hand it over to ``BeautifulSoup()``"]},{"cell_type":"code","execution_count":null,"id":"90f6e726","metadata":{"scrolled":true,"id":"90f6e726"},"outputs":[],"source":["# now save the source code\n","html = browser.page_source # get the source code\n","\n","from bs4 import BeautifulSoup\n","soup = BeautifulSoup(html)"]},{"cell_type":"markdown","id":"37c2ea46","metadata":{"id":"37c2ea46"},"source":["Let us try to extract some elements from the website, e.g. the headlines which hold the job titles."]},{"cell_type":"code","execution_count":null,"id":"6ae6d9eb","metadata":{"scrolled":true,"id":"6ae6d9eb"},"outputs":[],"source":["job_headlines = soup.select(\"ul.jobsearch-ResultsList li h2\")\n","headlines = [job.get_text() for job in job_headlines]\n","headlines"]},{"cell_type":"markdown","id":"0f130ae1","metadata":{"id":"0f130ae1"},"source":["Let's also store the links to the job descriptions."]},{"cell_type":"code","execution_count":null,"id":"e61ef87f","metadata":{"scrolled":false,"id":"e61ef87f"},"outputs":[],"source":["job_links = soup.select(\"ul.jobsearch-ResultsList li h2 a\") # \"a\" tag within \"h2\" tag within \"li\" tag within \"ul\" tag of class \"jobsearch-ResultsList\"\n","\n","urls = [link[\"href\"] for link in job_links]\n","\n","print(len(urls))\n","urls[0]"]},{"cell_type":"markdown","id":"bffce957","metadata":{"id":"bffce957"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Can you extract the information how many hits were found?"]},{"cell_type":"code","execution_count":null,"id":"0af64a8d","metadata":{"id":"0af64a8d"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ca484baf","metadata":{"id":"ca484baf"},"source":["---"]},{"cell_type":"markdown","id":"ba28f652","metadata":{"id":"ba28f652"},"source":["### Crawling through a list of links\n","\n","Let's assume we want to store information on all the jobs we found. Good practice would be to split the process into two steps:\n"," 1. Loop through all URLs to fetch and store the html source code\n"," 2. Extract the relevant information from the stored files\n","This way you do not need to bother the webserver more than once. This is also important because often you do not know beforehand what you actually need and might end up making an unessearly large number of requests otherwise.\n","\n","Let's start with step 1. To be able to store each html file we need to make sure of a few things:\n","\n"," - create a new directory on the file system (where we can store the files)\n"," - add the base url in front\n"," - think about the filenames to use\n"," - use a file handler that saves the pages to the filesystem\n"]},{"cell_type":"markdown","id":"3600db6e","metadata":{"id":"3600db6e"},"source":["><font color = 4e1585>SIDENOTE ON FILE HANDLERS: We can use Python's built-in ``open`` function to write to (or read from) different types of files (htlm, txt csv etc.). To create a new file, you must first open it in *write* mode (``w``) using the ``open`` function. They you can use the *write* method to write into your file:\n",">```python\n","my_file = open('page0.html', 'w')  # Open a (new) file in write (w) mode\n","my_file.write(myString)             # Write to file\n","my_file.close()                     # Close file\n","```"]},{"cell_type":"markdown","id":"9d00758f","metadata":{"id":"9d00758f"},"source":["><font color = 4e1585> A more elegant way to do the same looks as follows (closes the file automatically after the ``with`` block):\n",">```python\n","with open('page0.html', 'w') as my_file:\n","  my_file.write(myString)\n","```"]},{"cell_type":"markdown","id":"edfb9b86","metadata":{"id":"edfb9b86"},"source":["So let's change to the directory we created and then loop through all urls. For each job, we will append the url to the base url, fetch the source code and save it to our computer. For simplicity, we will save the files as job0, job1 etc. To get the index in each loop, we can use the ``enumerate()`` function."]},{"cell_type":"code","execution_count":null,"id":"3bc2dad9","metadata":{"scrolled":true,"id":"3bc2dad9"},"outputs":[],"source":["import os\n","#os.chdir(\"C:/Users/farys/Documents/indeed\")\n","os.chdir(r\"C:\\Users\\jakob\\Programming2\\indeed\")\n","#os.chdir(\"C:/Users/rudi/Documents/indeed\") # was created manually before! Use os.mkdir(path) to create a directory from within Python\n","\n","# Loop through all urls\n","for i, url in enumerate(urls):\n","    url = \"http://ch.indeed.com\" + url # Add base url in front\n","    browser.get(url) # Go to page\n","    with open('job' + str(i) + '.html', 'w', encoding=\"utf-8\") as file: # Open file in write mode\n","        file.write(browser.page_source) # Write source code of each page into file\n","    print(\"Stored page for job \" + str(i))"]},{"cell_type":"markdown","id":"aab3c25a","metadata":{"id":"aab3c25a"},"source":["**But wait, what if there are more than 15 hits?** So far, we have only retrieved the jobs that were displayed on the first page.\n","\n","We could make the browser click on the \"next\" button to create a list of the links to all the jobs (rather than just those on the first page):"]},{"cell_type":"code","execution_count":null,"id":"32ac3ab0","metadata":{"id":"32ac3ab0"},"outputs":[],"source":["# Re-initialize browser session\n","browser.quit() # Close the browser\n","browser = webdriver.Chrome()\n","browser.get('https://ch.indeed.com')\n","\n","elem = browser.find_element(By.ID, 'text-input-what')\n","elem.send_keys('data scientist')\n","elem = browser.find_element(By.ID, 'text-input-where')\n","#elem.clear()\n","elem.send_keys('Bern, BE' + Keys.RETURN)"]},{"cell_type":"markdown","id":"74bf9741","metadata":{"id":"74bf9741"},"source":["Let's first click on the button to accept cookies because it seems to get in our way when we want to click on \"next page\"."]},{"cell_type":"code","execution_count":null,"id":"9798b11d","metadata":{"id":"9798b11d"},"outputs":[],"source":["browser.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\").click()"]},{"cell_type":"markdown","id":"4fe42ea4","metadata":{"id":"4fe42ea4"},"source":["Now, we can write a loop that clicks on the ``>`` (next) button as long as this is possible and retrieves all the urls to the indiviual job descriptions. We can do this using an infinite while loop and and a try-except block. We also have to deal with a window inviting us to register that pops up on the second page."]},{"cell_type":"code","execution_count":null,"id":"afed4157","metadata":{"id":"afed4157"},"outputs":[],"source":["from selenium.common.exceptions import ElementClickInterceptedException\n","from selenium.common.exceptions import NoSuchElementException\n","\n","urls = [] # Start with empty list\n","while True:\n","    html = browser.page_source\n","    soup = BeautifulSoup(html)\n","    job_links = soup.select(\"ul.jobsearch-ResultsList li h2 a\") # retrieve \"a\" (link) tags\n","    page_urls = [link[\"href\"] for link in job_links] # Get \"href\" attribute of each link and write into a list\n","    urls += page_urls # add to url list\n","    try:\n","        elem = browser.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n","        elem.click()\n","    except ElementClickInterceptedException: # Close pop-up window that gets into our way\n","        browser.find_element(By.CSS_SELECTOR, \"button.icl-CloseButton\").click()\n","        elem = browser.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n","        elem.click()\n","    except NoSuchElementException: # Break on last page where > button does not exist\n","        break\n","\n","len(urls)"]},{"cell_type":"markdown","id":"39671701","metadata":{"id":"39671701"},"source":["Now that we have the complete list of URLs, we can scrape all the respective pages and save them to our computer:"]},{"cell_type":"code","execution_count":null,"id":"1d9ca5b7","metadata":{"scrolled":true,"id":"1d9ca5b7"},"outputs":[],"source":["for i, url in enumerate(urls):\n","    url = \"http://ch.indeed.com\" + url\n","    browser.get(url)\n","    with open('job' + str(i) + '.html', 'w', encoding=\"utf-8\") as file:\n","        file.write(browser.page_source)\n","        print(\"Stored page for job \" + str(i))"]},{"cell_type":"markdown","id":"57f75878","metadata":{"id":"57f75878"},"source":["><font color = 4e1585> SIDENOTE: *Indeed* might realize that you are not a normal user and ask you to verify that you are human. To make your scraper more robust to such problems (and make sure it is less of a burden to the page you are scraping), you could add some sleep time in each iteration. For example, ``time.sleep(10)`` (from the ``time`` module) would tell Python to wait for 10 seconds in each iteration."]},{"cell_type":"markdown","id":"4683e84e","metadata":{"id":"4683e84e"},"source":["\n","### Extracting information from html files\n","Now that we seperately stored the pages about the jobs we can proceed to step 2 and start to work with them. We would like to create a nice pandas dataframe with information on all the jobs we found. Suppose, we are interested in the job title and the employer. Let's define a function that extracts these elements and returns them in a list:"]},{"cell_type":"code","execution_count":null,"id":"42a7eaec","metadata":{"scrolled":true,"id":"42a7eaec"},"outputs":[],"source":["# define a function that extracts the elements we want from the files with source code for each page\n","def getStuff(page):\n","    with open(page, encoding = \"utf-8\") as file:\n","        content = file.read() # Open file in read mode and assign to variable \"content\"\n","\n","    soup = BeautifulSoup(content)\n","\n","    # extract elements we like\n","    jobtitle = soup.select(\"h1 span\")[0].get_text()\n","    employer = soup.select(\"div[data-company-name='true'] a\")[0].get_text()\n","    return [jobtitle, employer]"]},{"cell_type":"markdown","id":"b6aa3cff","metadata":{"id":"b6aa3cff"},"source":["Now, let's loop through all the html files, apply our function and write everything into a nested list:"]},{"cell_type":"code","execution_count":null,"id":"7f18e730","metadata":{"id":"7f18e730"},"outputs":[],"source":["# Get names of all files\n","pages = os.listdir()\n","\n","# Loop through html files\n","job_summary = []\n","for page in pages:\n","    try:\n","        job_summary.append(getStuff(page))\n","    except:\n","        print(\"Problem with file\", page)"]},{"cell_type":"markdown","id":"af9f49bb","metadata":{"id":"af9f49bb"},"source":["Finally, we can convert our nested list into a Pandas Dataframe."]},{"cell_type":"code","execution_count":null,"id":"24a0e870","metadata":{"id":"24a0e870"},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame.from_records(job_summary, columns = [\"jobtitle\", \"employer\"])\n","df"]},{"cell_type":"markdown","id":"4ebdfebf","metadata":{"id":"4ebdfebf"},"source":["### What to do from here?\n","\n","We wanted to give you some first impressions of what ``selenium`` is capable of -- but there is much more to learn! You could extend/finetune our example project in different ways:\n","\n"," * make sure that each site is dowloaded correctly and only once:\n","   - check if file exists on the system and is not empty (e.g. if exists(somefile): skip downloading)\n","   - use better filenames, e.g. based on the ID (jk=...) to make this check easier\n","   - introduce a short wait time in your loop if necessary (selenium can even wait/check until a certain element is present)\n"," * extract other elements:\n","   - skills\n","   - full texts\n","   - ...\n"," * Split your code into two separate scripts (one for data collection and one for data processing)\n"," * Think about a strategy to extend the search terms and/or locations\n"," * Make your scaper check for new job openings once a week\n"," * ...\n","\n","You might like to check out the following tutorials for more ideas: https://youtube.com/playlist?list=PLzMcBGfZo4-n40rB1XaJ0ak1bemvlqumQ"]},{"cell_type":"markdown","id":"4c6f0b57","metadata":{"id":"4c6f0b57"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Using the files you saved to your computer, create a pandas dataframe that contains not only the job title and the employer, but, also the place (Thun, Bern etc.)."]},{"cell_type":"code","execution_count":null,"id":"6395e1fb","metadata":{"id":"6395e1fb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"63820c7e","metadata":{"id":"63820c7e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d11dfb37","metadata":{"id":"d11dfb37"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b70a50f3","metadata":{"id":"b70a50f3"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}