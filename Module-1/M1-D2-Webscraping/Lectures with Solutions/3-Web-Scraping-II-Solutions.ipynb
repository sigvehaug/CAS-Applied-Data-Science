{"cells":[{"cell_type":"markdown","id":"c6378169","metadata":{"id":"c6378169"},"source":["# Web Scraping II\n","#### CAS Applied Data Science 2025 ####\n","\n","\n","In the previous tutorials, we learned how to automatically retrieve pages from the internet through **web scraping**. We sent HTTP requests with the  ``requests`` library and used the ``BeautifulSoup`` library to parse and work with the HTML code from the response we got. This is a good start and works well for so-called **static** pages, but when scraping **dynamic** websites that use JavaScript in the background, you will notice that this approach often fails. Today, we will learn how to deal with this problem."]},{"cell_type":"markdown","id":"dd74906f","metadata":{"id":"dd74906f"},"source":["## Scraping dynamic websites\n","\n","Imagine you want to scrape all currently listed open positions of \"data scientists\" in \"Bern\" on www.indeed.com.\n","\n","Let's have a look at the output in a browser: https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE\n","\n","We could load the website with ``requests`` and extract the parts that we want using ``BeautifulSoup``:"]},{"cell_type":"code","execution_count":1,"id":"498677f1","metadata":{"scrolled":true,"id":"498677f1","outputId":"500f091c-ce41-48c3-e390-251dfe82b969","executionInfo":{"status":"ok","timestamp":1755620781017,"user_tz":-120,"elapsed":696,"user":{"displayName":"Sebastian Heinrich","userId":"17447258035648262756"}},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<p id=\"paragraph\"></p>,\n"," <p><a href=\"https://support.indeed.com/hc/en-us/articles/33465379855501-Troubleshooting-Cloudflare-Errors\" id=\"troubleshooting\">Troubleshooting Cloudflare Errors</a></p>,\n"," <p><span id=\"needHelp\">Need more help?</span> <a href=\"https://www.indeed.com/support/contact\" id=\"contactUs\">Contact us</a></p>]"]},"metadata":{},"execution_count":1}],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","res = requests.get(\"https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE&vjk=4d0278f36b754f74\")\n","res.text\n","\n","# information sits in <li> elements under <ul class='jobsearch-ResultsList'>, but there might be other ways to get to it too...\n","soup = BeautifulSoup(res.text)\n","soup\n","soup.find_all(\"p\")"]},{"cell_type":"markdown","id":"bc983825","metadata":{"id":"bc983825"},"source":["That looks bad. The webserver recognises that we are not a regular user entering normally through a browser. What could we do from here?"]},{"cell_type":"code","execution_count":null,"id":"7821a216","metadata":{"scrolled":false,"id":"7821a216","outputId":"54b063b0-8a10-4f2a-ac91-3c495221c2c9"},"outputs":[{"data":{"text/plain":["[<p>You do not have access to ch.indeed.com.</p>,\n"," <p>The site owner may have set restrictions that prevent you from accessing the site.</p>,\n"," <p class=\"cf-dropdown-title\">Error details</p>,\n"," <p class=\"cf-error-copy-description\">Provide the site owner this information.</p>,\n"," <p class=\"cf-error-details-endpoint\">I got an error when visiting ch.indeed.com/jobs?q=data+scientist&amp;l=Bern%2C+BE&amp;vjk=4d0278f36b754f74.</p>,\n"," <p>Error code: 1020</p>,\n"," <p>Ray ID: 7be7b9bc8b51ce9f</p>,\n"," <p>Country: CH</p>,\n"," <p>Data center: gva01</p>,\n"," <p>IP: 130.92.178.236</p>,\n"," <p>Timestamp: 2023-04-27 14:23:49 UTC</p>,\n"," <p class=\"cf-copy-label hidden\" id=\"copy-label\">Click to copy</p>,\n"," <p>\n","                Performance &amp; security by <a href=\"https://www.cloudflare.com?utm_source=1020_error\" rel=\"noopener noreferrer\" target=\"_blank\">Cloudflare <img alt=\"External link\" class=\"external-link\" src=\"/cdn-cgi/images/external.png\" title=\"Opens in new tab\"/></a>\n"," </p>]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["url = 'https://ch.indeed.com/jobs?q=data+scientist&l=Bern%2C+BE'\n","\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n","}\n","\n","response = requests.get(url, headers=headers)\n","soup = BeautifulSoup(res.text)\n","\n","soup.find_all(\"p\")"]},{"cell_type":"markdown","id":"4e2d5f82","metadata":{"id":"4e2d5f82"},"source":["Still not working. What is going on here? If the html source code you retrieve from a page does not correspond to what you see when viewing this page in your browser, you are probably dealing with a **dynamic web page**. Typically, this means that some JavaScript code is executed in the background to \"create\" the source code of the final page you see. When you scrape a dynamic page with the requests library, you will not get the final html code, but often an incomplete version where you might only see the instructions to run the JavaScript files. Some web pages also use dynamic features to actively prevent web scraping. You might then get an error message like the one in the example above. Luckily, there is another Python package we can use to address such problems."]},{"cell_type":"markdown","id":"e323dea8","metadata":{"id":"e323dea8"},"source":["## Selenium\n","\n","``selenium`` is a Java based software that enables you to automate browsers (e.g. Chrome, Firefox etc.). You can think of it as an interface that allows you to control what your browser does through code. It was developed for automated website-testing, but is also very useful for scraping web pages, especially those that require JavaScript rendering. It can automate web browsing and interactions, like clicking buttons or filling out forms, and supports a wide range of browsers, including Firefox, Chrome, and Edge. Selenium can be used in multiple programming languages including Python. However, it might require an installation of browser drivers as well as Java and can sometimes be a bit tedious to set up.\n","\n","Let's start by installing the ``selenium`` Python package.\n","\n","**Note that you cannot use ``selenium`` from within Colab as the Python instance is running in the cloud where no browser is available. Copy the notbook of this tutorial to your local machine and run the code in, e.g., Jupyter Notebook.**"]},{"cell_type":"code","execution_count":null,"id":"52a6f130","metadata":{"id":"52a6f130","outputId":"09acb40f-e6d7-4470-fe5a-3816002198d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: selenium in c:\\users\\jakob\\anaconda3\\lib\\site-packages (4.9.0)\n","Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n","Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n","Requirement already satisfied: trio~=0.17 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n","Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n","Requirement already satisfied: sniffio in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n","Requirement already satisfied: idna in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n","Requirement already satisfied: outcome in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n","Requirement already satisfied: sortedcontainers in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: async-generator>=1.9 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n","Requirement already satisfied: cffi>=1.14 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n","Requirement already satisfied: attrs>=19.2.0 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n","Requirement already satisfied: pycparser in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n","Requirement already satisfied: wsproto>=0.14 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\jakob\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n"]}],"source":["# Selenium can be installed via pip\n","!pip install selenium"]},{"cell_type":"markdown","id":"9677efbe","metadata":{"id":"9677efbe"},"source":["Selenium requires a driver (e.g. chromedriver) to communicate with your favorite browser (e.g. chrome). In the newest version of the selenium module, webdrivers for different browsers should be installed automatically. If this is not the case, you can install them manually.\n","\n","**A list of available drivers:**\n","\n","* Chrome:\n"," - https://chromedriver.chromium.org/downloads\n","\n","* Edge:\n"," - https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n","\n","* Firefox:\n"," - https://github.com/mozilla/geckodriver/releases\n","\n","* Safari:\n"," - https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n","\n","If the code below not work, make sure you have an appropriate driver (matching version) installed and that the driver is accessible, i.e. it is located in your ``PATH`` environment. If you want to know where the selenium package was stored on your computer, you can import it and then type ``print(selenium.__file__)``.\n","\n","You can read up setup instructions here: https://pypi.org/project/selenium/\n","\n","Let's try to import the webdriver, initialize it and got to 'https://ch.indeed.com' (i.e. send a get request). WE will also import  \n"]},{"cell_type":"code","execution_count":null,"id":"11bc1f42","metadata":{"id":"11bc1f42"},"outputs":[],"source":["from selenium import webdriver\n","browser = webdriver.Chrome()\n","browser.get('https://ch.indeed.com')"]},{"cell_type":"markdown","id":"bb65d4d5","metadata":{"id":"bb65d4d5"},"source":["This opens your preferred browser. With selenium you can now take control of what the browser does, e.g.\n","* fill in textfields,\n","* click on elements,\n","* scroll,\n","* etc.\n"]},{"cell_type":"markdown","id":"ee258d7e","metadata":{"id":"ee258d7e"},"source":["><font color = 4e1585> SIDENOTE: If the version of the driver and the version of your browser do not match, you will get an error. One way to address it is by using selenium's ``webdriver_manager``, which allows you to automatically install the driver version that corresponds to your browser (and also bypasses problems regarding the PATH environment):\n",">```python  \n","!pip install webdriver_manager\n","from selenium import webdriver\n","from webdriver_manager.chrome import ChromeDriverManager\n","browser = webdriver.Chrome(ChromeDriverManager().install())\n","```"]},{"cell_type":"markdown","id":"ffca01ab","metadata":{"id":"ffca01ab"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Use Jupyter or an IDE (Spyder/pyCharm). Install ``selenium`` and try to start a selenium controlled browser window (Chrome/Firefox/Edge/Safari). If it does not immediately work, don't panic. Depending on your operating system, python version, selenium version, browser version etc. all kind of problems may occur! If it does not quickly work, take your time and try to find the problem after the course. Also note: we might get blocked from indeed when we do this in the course.\n","<font color='teal'> Make sure that:\n","\n","* <font color='teal'>Both browser and selenium is the current version (e.g. force an update if you have an old version of selenium)\n","* <font color='teal'>and/or update your browser\n","* <font color='teal'>Depending on version and system you might need to manually install the driver for your browser.\n","* <font color='teal'>If the verions of your driver and your brwoser don't match, you can use selenium's built-in functionality to install a driver via the ``webdriver_manager`` (see sidenote above).\n"]},{"cell_type":"code","execution_count":null,"id":"95ccb195","metadata":{"id":"95ccb195","outputId":"4401bbaa-0ecf-460f-b949-f67f98d9c042"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\jakob\\AppData\\Local\\Temp/ipykernel_1664/3030008906.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  browser = webdriver.Chrome(ChromeDriverManager().install())\n"]}],"source":["# Most robust solution (with package manager)\n","from selenium import webdriver\n","from webdriver_manager.chrome import ChromeDriverManager\n","browser = webdriver.Chrome(ChromeDriverManager().install())\n","browser.get('https://ch.indeed.com')"]},{"cell_type":"markdown","id":"b57ae5ec","metadata":{"id":"b57ae5ec"},"source":["\n","---\n"]},{"cell_type":"markdown","id":"36aa733d","metadata":{"id":"36aa733d"},"source":["### Filling in text fields\n","\n","Now that we have a browser window running that is controlled by our Python code, selenium allows us to do many different things (see here for all available methods: https://www.simplilearn.com/tutorials/python-tutorial/selenium-with-python). For example, we could send keystrokes to the searchbar.\n","\n","We want to search for positions for data scientists. First, we need to locate the search fields. This can be done using the ``find_element`` method. It allows us to find matching elements on a website based on locator values such as the tage name, (ByTAG_NAME), id (By.ID), the class (By.Class), css selectors (By.CSS) and much more. If we inspect the source code of the page, we will see that the id of the tag containing the \"What\" field (for the job title) is \"text-input-what\" while the one for the \"Where\" field is 'text-input-where'. Let's start by locating the \"What\" field:"]},{"cell_type":"code","execution_count":null,"id":"bfbbae74","metadata":{"id":"bfbbae74"},"outputs":[],"source":["from selenium.webdriver.common.by import By # To find elements\n","from selenium.webdriver.common.keys import Keys # For special keys (Enter, delete, down etc.)\n","\n","#elem = browser.find_element(By.CSS_SELECTOR, \"[type='checkbox']\")\n","#elem.click()"]},{"cell_type":"code","source":["elem = browser.find_element(By.ID, 'text-input-what') # Find element"],"metadata":{"id":"CWJeLqesP9TK"},"id":"CWJeLqesP9TK","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"43864e63","metadata":{"id":"43864e63"},"source":["Now we can send the keytrokes to the selected field:"]},{"cell_type":"code","execution_count":null,"id":"4fde9692","metadata":{"id":"4fde9692"},"outputs":[],"source":["elem.send_keys('data scientist') # enter search query for \"data scientist\""]},{"cell_type":"markdown","id":"ed9ec400","metadata":{"id":"ed9ec400"},"source":["Let's limit our search to Bern and hit Enter to get the job openings:"]},{"cell_type":"code","execution_count":null,"id":"109f8d71","metadata":{"id":"109f8d71"},"outputs":[],"source":["elem = browser.find_element(By.ID, 'text-input-where') # find \"where\" field\n","elem.send_keys('Bern, BE' + Keys.RETURN) # enter \"Bern. BE\" and hit Enter!"]},{"cell_type":"markdown","id":"b78d8c1c","metadata":{"id":"b78d8c1c"},"source":["---\n","\n"," <font color='teal'> **In-class exercise**:\n","Can you click (method ``.click()``) on the drop down menu for the language?"]},{"cell_type":"code","execution_count":null,"id":"df2b46f4","metadata":{"id":"df2b46f4"},"outputs":[],"source":["browser.find_element(By.ID, 'filter-lang').click()"]},{"cell_type":"markdown","id":"95f9b51f","metadata":{"id":"95f9b51f"},"source":["---"]},{"cell_type":"markdown","id":"efbc60fc","metadata":{"id":"efbc60fc"},"source":["### Extracting elements\n","\n","From here we can fetch the pages source code and hand it over to ``BeautifulSoup()``"]},{"cell_type":"code","execution_count":null,"id":"90f6e726","metadata":{"scrolled":true,"id":"90f6e726"},"outputs":[],"source":["# now save the source code\n","html = browser.page_source # get the source code\n","\n","from bs4 import BeautifulSoup\n","soup = BeautifulSoup(html)"]},{"cell_type":"markdown","id":"37c2ea46","metadata":{"id":"37c2ea46"},"source":["Let us try to extract some elements from the website, e.g. the headlines which hold the job titles."]},{"cell_type":"code","execution_count":null,"id":"6ae6d9eb","metadata":{"scrolled":true,"id":"6ae6d9eb","outputId":"2648aa56-df3d-4e43-974b-be97dbb06a45"},"outputs":[{"data":{"text/plain":["['Senior Data Scientist',\n"," 'Data Scientist, Data Analyst',\n"," 'Biodata Scientist (w/m/d)',\n"," 'Data Scientist',\n"," 'Downstream Process Scientist',\n"," 'ICT Data Scientist GIS 80 - 100%',\n"," 'Assistant to the Communication and Project Officer',\n"," 'Senior Data Scientist 80%-100%',\n"," 'ICT Data Scientist GIS',\n"," 'Research associate/assistant (f/m/d)',\n"," 'Assistant to the Communication and Project Officer',\n"," 'Data Engineer (Big Data)',\n"," 'Instrument Scientist Space Research / Planetary Sciences',\n"," 'Bioinformatician/Computational Biologist 100%',\n"," 'Bioinformatician/Computational Biologist 100%']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["job_headlines = soup.select(\"ul.jobsearch-ResultsList li h2\")\n","headlines = [job.get_text() for job in job_headlines]\n","headlines"]},{"cell_type":"markdown","id":"0f130ae1","metadata":{"id":"0f130ae1"},"source":["Let's also store the links to the job descriptions."]},{"cell_type":"code","execution_count":null,"id":"e61ef87f","metadata":{"scrolled":false,"id":"e61ef87f","outputId":"804e1e5c-ce4d-4578-d482-cdb3043867b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["15\n"]},{"data":{"text/plain":["'/rc/clk?jk=a68c6dc9327501f4&fccid=ace7b2d91ad22f12&vjs=3'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["job_links = soup.select(\"ul.jobsearch-ResultsList li h2 a\") # \"a\" tag within \"h2\" tag within \"li\" tag within \"ul\" tag of class \"jobsearch-ResultsList\"\n","\n","urls = [link[\"href\"] for link in job_links]\n","\n","print(len(urls))\n","urls[0]"]},{"cell_type":"markdown","id":"bffce957","metadata":{"id":"bffce957"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Can you extract the information how many hits were found?"]},{"cell_type":"code","execution_count":null,"id":"0af64a8d","metadata":{"id":"0af64a8d","outputId":"752d6a71-8af6-4da3-c526-186f76a5a244"},"outputs":[{"data":{"text/plain":["'34 jobs'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["soup.select(\".jobsearch-JobCountAndSortPane-jobCount\")[0].get_text()"]},{"cell_type":"markdown","id":"ca484baf","metadata":{"id":"ca484baf"},"source":["---"]},{"cell_type":"markdown","id":"ba28f652","metadata":{"id":"ba28f652"},"source":["### Crawling through a list of links\n","\n","Let's assume we want to store information on all the jobs we found. Good practice would be to split the process into two steps:\n"," 1. Loop through all URLs to fetch and store the html source code\n"," 2. Extract the relevant information from the stored files\n","This way you do not need to bother the webserver more than once. This is also important because often you do not know beforehand what you actually need and might end up making an unessearly large number of requests otherwise.\n","\n","Let's start with step 1. To be able to store each html file we need to make sure of a few things:\n","\n"," - create a new directory on the file system (where we can store the files)\n"," - add the base url in front\n"," - think about the filenames to use\n"," - use a file handler that saves the pages to the filesystem\n"]},{"cell_type":"markdown","id":"3600db6e","metadata":{"id":"3600db6e"},"source":["><font color = 4e1585>SIDENOTE ON FILE HANDLERS: We can use Python's built-in ``open`` function to write to (or read from) different types of files (htlm, txt csv etc.). To create a new file, you must first open it in *write* mode (``w``) using the ``open`` function. They you can use the *write* method to write into your file:\n",">```python\n","my_file = open('page0.html', 'w')  # Open a (new) file in write (w) mode\n","my_file.write(myString)             # Write to file\n","my_file.close()                     # Close file\n","```"]},{"cell_type":"markdown","id":"9d00758f","metadata":{"id":"9d00758f"},"source":["><font color = 4e1585> A more elegant way to do the same looks as follows (closes the file automatically after the ``with`` block):\n",">```python\n","with open('page0.html', 'w') as my_file:\n","  my_file.write(myString)\n","```"]},{"cell_type":"markdown","id":"edfb9b86","metadata":{"id":"edfb9b86"},"source":["So let's change to the directory we created and then loop through all urls. For each job, we will append the url to the base url, fetch the source code and save it to our computer. For simplicity, we will save the files as job0, job1 etc. To get the index in each loop, we can use the ``enumerate()`` function."]},{"cell_type":"code","execution_count":null,"id":"3bc2dad9","metadata":{"scrolled":true,"id":"3bc2dad9","outputId":"c24ba9ac-c20b-4e98-b632-6e362bc11732"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stored page for job 0\n","Stored page for job 1\n","Stored page for job 2\n","Stored page for job 3\n","Stored page for job 4\n","Stored page for job 5\n","Stored page for job 6\n","Stored page for job 7\n","Stored page for job 8\n","Stored page for job 9\n","Stored page for job 10\n","Stored page for job 11\n","Stored page for job 12\n","Stored page for job 13\n","Stored page for job 14\n"]}],"source":["import os\n","#os.chdir(\"C:/Users/farys/Documents/indeed\")\n","os.chdir(r\"C:\\Users\\jakob\\Programming2\\indeed\")\n","#os.chdir(\"C:/Users/rudi/Documents/indeed\") # was created manually before! Use os.mkdir(path) to create a directory from within Python\n","\n","# Loop through all urls\n","for i, url in enumerate(urls):\n","    url = \"http://ch.indeed.com\" + url # Add base url in front\n","    browser.get(url) # Go to page\n","    with open('job' + str(i) + '.html', 'w', encoding=\"utf-8\") as file: # Open file in write mode\n","        file.write(browser.page_source) # Write source code of each page into file\n","    print(\"Stored page for job \" + str(i))"]},{"cell_type":"markdown","id":"aab3c25a","metadata":{"id":"aab3c25a"},"source":["**But wait, what if there are more than 15 hits?** So far, we have only retrieved the jobs that were displayed on the first page.\n","\n","We could make the browser click on the \"next\" button to create a list of the links to all the jobs (rather than just those on the first page):"]},{"cell_type":"code","execution_count":null,"id":"32ac3ab0","metadata":{"id":"32ac3ab0"},"outputs":[],"source":["# Re-initialize browser session\n","browser.quit() # Close the browser\n","browser = webdriver.Chrome()\n","browser.get('https://ch.indeed.com')\n","\n","elem = browser.find_element(By.ID, 'text-input-what')\n","elem.send_keys('data scientist')\n","elem = browser.find_element(By.ID, 'text-input-where')\n","#elem.clear()\n","elem.send_keys('Bern, BE' + Keys.RETURN)"]},{"cell_type":"markdown","id":"74bf9741","metadata":{"id":"74bf9741"},"source":["Let's first click on the button to accept cookies because it seems to get in our way when we want to click on \"next page\"."]},{"cell_type":"code","execution_count":null,"id":"9798b11d","metadata":{"id":"9798b11d"},"outputs":[],"source":["browser.find_element(By.CSS_SELECTOR, \"#onetrust-accept-btn-handler\").click()"]},{"cell_type":"markdown","id":"4fe42ea4","metadata":{"id":"4fe42ea4"},"source":["Now, we can write a loop that clicks on the ``>`` (next) button as long as this is possible and retrieves all the urls to the indiviual job descriptions. We can do this using an infinite while loop and and a try-except block. We also have to deal with a window inviting us to register that pops up on the second page."]},{"cell_type":"code","execution_count":null,"id":"afed4157","metadata":{"id":"afed4157","outputId":"0957b904-1217-4e1e-dbdb-1431d9312b25"},"outputs":[{"data":{"text/plain":["30"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from selenium.common.exceptions import ElementClickInterceptedException\n","from selenium.common.exceptions import NoSuchElementException\n","\n","urls = [] # Start with empty list\n","while True:\n","    html = browser.page_source\n","    soup = BeautifulSoup(html)\n","    job_links = soup.select(\"ul.jobsearch-ResultsList li h2 a\") # retrieve \"a\" (link) tags\n","    page_urls = [link[\"href\"] for link in job_links] # Get \"href\" attribute of each link and write into a list\n","    urls += page_urls # add to url list\n","    try:\n","        elem = browser.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n","        elem.click()\n","    except ElementClickInterceptedException: # Close pop-up window that gets into our way\n","        browser.find_element(By.CSS_SELECTOR, \"button.icl-CloseButton\").click()\n","        elem = browser.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n","        elem.click()\n","    except NoSuchElementException: # Break on last page where > button does not exist\n","        break\n","\n","len(urls)"]},{"cell_type":"markdown","id":"39671701","metadata":{"id":"39671701"},"source":["Now that we have the complete list of URLs, we can scrape all the respective pages and save them to our computer:"]},{"cell_type":"code","execution_count":null,"id":"1d9ca5b7","metadata":{"scrolled":true,"id":"1d9ca5b7","outputId":"f8653850-8d5f-4fc3-d5df-ec4cf04e49c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stored page for job 0\n","Stored page for job 1\n","Stored page for job 2\n","Stored page for job 3\n","Stored page for job 4\n","Stored page for job 5\n","Stored page for job 6\n","Stored page for job 7\n","Stored page for job 8\n","Stored page for job 9\n","Stored page for job 10\n","Stored page for job 11\n","Stored page for job 12\n","Stored page for job 13\n","Stored page for job 14\n","Stored page for job 15\n","Stored page for job 16\n","Stored page for job 17\n","Stored page for job 18\n","Stored page for job 19\n","Stored page for job 20\n","Stored page for job 21\n","Stored page for job 22\n","Stored page for job 23\n","Stored page for job 24\n","Stored page for job 25\n","Stored page for job 26\n","Stored page for job 27\n","Stored page for job 28\n","Stored page for job 29\n"]}],"source":["for i, url in enumerate(urls):\n","    url = \"http://ch.indeed.com\" + url\n","    browser.get(url)\n","    with open('job' + str(i) + '.html', 'w', encoding=\"utf-8\") as file:\n","        file.write(browser.page_source)\n","        print(\"Stored page for job \" + str(i))"]},{"cell_type":"markdown","id":"57f75878","metadata":{"id":"57f75878"},"source":["><font color = 4e1585> SIDENOTE: *Indeed* might realize that you are not a normal user and ask you to verify that you are human. To make your scraper more robust to such problems (and make sure it is less of a burden to the page you are scraping), you could add some sleep time in each iteration. For example, ``time.sleep(10)`` (from the ``time`` module) would tell Python to wait for 10 seconds in each iteration."]},{"cell_type":"markdown","id":"4683e84e","metadata":{"id":"4683e84e"},"source":["\n","### Extracting information from html files\n","Now that we seperately stored the pages about the jobs we can proceed to step 2 and start to work with them. We would like to create a nice pandas dataframe with information on all the jobs we found. Suppose, we are interested in the job title and the employer. Let's define a function that extracts these elements and returns them in a list:"]},{"cell_type":"code","execution_count":null,"id":"42a7eaec","metadata":{"scrolled":true,"id":"42a7eaec"},"outputs":[],"source":["# define a function that extracts the elements we want from the files with source code for each page\n","def getStuff(page):\n","    with open(page, encoding = \"utf-8\") as file:\n","        content = file.read() # Open file in read mode and assign to variable \"content\"\n","\n","    soup = BeautifulSoup(content)\n","\n","    # extract elements we like\n","    jobtitle = soup.select(\"h1 span\")[0].get_text()\n","    employer = soup.select(\"div[data-company-name='true'] a\")[0].get_text()\n","    return [jobtitle, employer]"]},{"cell_type":"markdown","id":"b6aa3cff","metadata":{"id":"b6aa3cff"},"source":["Now, let's loop through all the html files, apply our function and write everything into a nested list:"]},{"cell_type":"code","execution_count":null,"id":"7f18e730","metadata":{"id":"7f18e730","outputId":"a211c960-fcd5-49e1-a124-638beb092678"},"outputs":[{"name":"stdout","output_type":"stream","text":["Problem with file job28.html\n","Problem with file job29.html\n","Problem with file job32.html\n"]}],"source":["# Get names of all files\n","pages = os.listdir()\n","\n","# Loop through html files\n","job_summary = []\n","for page in pages:\n","    try:\n","        job_summary.append(getStuff(page))\n","    except:\n","        print(\"Problem with file\", page)"]},{"cell_type":"markdown","id":"af9f49bb","metadata":{"id":"af9f49bb"},"source":["Finally, we can convert our nested list into a Pandas Dataframe."]},{"cell_type":"code","execution_count":null,"id":"24a0e870","metadata":{"id":"24a0e870","outputId":"5c70ff53-bad7-4197-aac8-18af6710d572"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>jobtitle</th>\n","      <th>employer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Senior Data Scientist</td>\n","      <td>BLS AG</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Data Scientist, Data Analyst</td>\n","      <td>Das Bundesamt für Gesundheit BAG</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Assistant to the Communication and Project Off...</td>\n","      <td>Universität Bern Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Data Engineer (Big Data)</td>\n","      <td>Fincons Group</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Instrument Scientist Space Research / Planetar...</td>\n","      <td>Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Bioinformatician/Computational Biologist 100%</td>\n","      <td>Universität Bern Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Bioinformatician/Computational Biologist 100%</td>\n","      <td>Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Downstream Process Scientist</td>\n","      <td>Hobson Prior</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Scientific Research Associate - R&amp;D (m/f/d)</td>\n","      <td>IDEXX</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Instrument Scientist Space Research / Planetar...</td>\n","      <td>Universität Bern Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Postdoc (f/m/d)</td>\n","      <td>Insel Gruppe</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Full Professorship in Translational Pharmacolo...</td>\n","      <td>Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Biodata Scientist (w/m/d)</td>\n","      <td>Insel Gruppe</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>PhD</td>\n","      <td>Insel Gruppe</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Senior Data Engineer</td>\n","      <td>mimacom</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Business Intelligence Analyst and Requirements...</td>\n","      <td>Schweizerischer Nationalfonds</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Security Operations Engineer / DevOps Engineer</td>\n","      <td>AdNovum</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Application Scientist in Process Analysis, Dev...</td>\n","      <td>TOFWERK AG</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Senior Data Engineer</td>\n","      <td>mimacom ag</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Jun./Prof. Entwickler im Big Data Umfeld</td>\n","      <td>Future IT GmbH</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>IT Business Analyst (m/w/d)</td>\n","      <td>Data Science Jobs</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Data Scientist</td>\n","      <td>Data Science Jobs</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Jun./Prof. Entwickler im Big Data Umfeld</td>\n","      <td>Future IT GmbH</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Security Operations Engineer / DevOps Engineer</td>\n","      <td>AdNovum Informatik</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Downstream Process Scientist</td>\n","      <td>We make it</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>ICT Data Scientist GIS 80 - 100%</td>\n","      <td>Energie Wasser Bern</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Assistant to the Communication and Project Off...</td>\n","      <td>Universität Bern</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Senior Data Scientist 80%-100%</td>\n","      <td>Data Science Jobs</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>ICT Data Scientist GIS</td>\n","      <td>ewb Energie Wasser Bern</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Research associate/assistant (f/m/d)</td>\n","      <td>Insel Gruppe</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             jobtitle  \\\n","0                               Senior Data Scientist   \n","1                        Data Scientist, Data Analyst   \n","2   Assistant to the Communication and Project Off...   \n","3                            Data Engineer (Big Data)   \n","4   Instrument Scientist Space Research / Planetar...   \n","5       Bioinformatician/Computational Biologist 100%   \n","6       Bioinformatician/Computational Biologist 100%   \n","7                        Downstream Process Scientist   \n","8         Scientific Research Associate - R&D (m/f/d)   \n","9   Instrument Scientist Space Research / Planetar...   \n","10                                    Postdoc (f/m/d)   \n","11  Full Professorship in Translational Pharmacolo...   \n","12                          Biodata Scientist (w/m/d)   \n","13                                                PhD   \n","14                               Senior Data Engineer   \n","15  Business Intelligence Analyst and Requirements...   \n","16     Security Operations Engineer / DevOps Engineer   \n","17  Application Scientist in Process Analysis, Dev...   \n","18                               Senior Data Engineer   \n","19           Jun./Prof. Entwickler im Big Data Umfeld   \n","20                        IT Business Analyst (m/w/d)   \n","21                                     Data Scientist   \n","22           Jun./Prof. Entwickler im Big Data Umfeld   \n","23     Security Operations Engineer / DevOps Engineer   \n","24                       Downstream Process Scientist   \n","25                   ICT Data Scientist GIS 80 - 100%   \n","26  Assistant to the Communication and Project Off...   \n","27                     Senior Data Scientist 80%-100%   \n","28                             ICT Data Scientist GIS   \n","29               Research associate/assistant (f/m/d)   \n","\n","                             employer  \n","0                              BLS AG  \n","1    Das Bundesamt für Gesundheit BAG  \n","2   Universität Bern Universität Bern  \n","3                       Fincons Group  \n","4                    Universität Bern  \n","5   Universität Bern Universität Bern  \n","6                    Universität Bern  \n","7                        Hobson Prior  \n","8                               IDEXX  \n","9   Universität Bern Universität Bern  \n","10                       Insel Gruppe  \n","11                   Universität Bern  \n","12                       Insel Gruppe  \n","13                       Insel Gruppe  \n","14                            mimacom  \n","15      Schweizerischer Nationalfonds  \n","16                            AdNovum  \n","17                         TOFWERK AG  \n","18                         mimacom ag  \n","19                     Future IT GmbH  \n","20                  Data Science Jobs  \n","21                  Data Science Jobs  \n","22                     Future IT GmbH  \n","23                 AdNovum Informatik  \n","24                         We make it  \n","25                Energie Wasser Bern  \n","26                   Universität Bern  \n","27                  Data Science Jobs  \n","28            ewb Energie Wasser Bern  \n","29                       Insel Gruppe  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","df = pd.DataFrame.from_records(job_summary, columns = [\"jobtitle\", \"employer\"])\n","df"]},{"cell_type":"markdown","id":"4ebdfebf","metadata":{"id":"4ebdfebf"},"source":["### What to do from here?\n","\n","We wanted to give you some first impressions of what ``selenium`` is capable of -- but there is much more to learn! You could extend/finetune our example project in different ways:\n","\n"," * make sure that each site is dowloaded correctly and only once:\n","   - check if file exists on the system and is not empty (e.g. if exists(somefile): skip downloading)\n","   - use better filenames, e.g. based on the ID (jk=...) to make this check easier\n","   - introduce a short wait time in your loop if necessary (selenium can even wait/check until a certain element is present)\n"," * extract other elements:\n","   - skills\n","   - full texts\n","   - ...\n"," * Split your code into two separate scripts (one for data collection and one for data processing)\n"," * Think about a strategy to extend the search terms and/or locations\n"," * Make your scaper check for new job openings once a week\n"," * ...\n","\n","You might like to check out the following tutorials for more ideas: https://youtube.com/playlist?list=PLzMcBGfZo4-n40rB1XaJ0ak1bemvlqumQ"]},{"cell_type":"markdown","id":"4c6f0b57","metadata":{"id":"4c6f0b57"},"source":["---\n","\n","<font color='teal'> **In-class exercise**:\n","Using the files you saved to your computer, create a pandas dataframe that contains not only the job title and the employer, but, also the place (Thun, Bern etc.)."]},{"cell_type":"code","execution_count":null,"id":"6395e1fb","metadata":{"id":"6395e1fb"},"outputs":[],"source":["# Check with one example\n","with open(\"job1.html\", encoding = \"utf-8\") as file:\n","    content = file.read()\n","soup = BeautifulSoup(content)\n","soup"]},{"cell_type":"code","execution_count":null,"id":"63820c7e","metadata":{"id":"63820c7e"},"outputs":[],"source":["# Adapt function\n","def getStuff(page):\n","    with open(page, encoding = \"utf-8\") as file:\n","        content = file.read()\n","\n","    soup = BeautifulSoup(content)\n","\n","    # extract elements we like\n","    jobtitle = soup.select(\"h1 span\")[0].get_text()\n","    employer = soup.select(\"div[data-company-name='true'] a\")[0].get_text()\n","    place = soup.select(\"div.css-6z8o9s\")[0].get_text()\n","\n","    return [jobtitle, employer, place]"]},{"cell_type":"code","execution_count":null,"id":"d11dfb37","metadata":{"id":"d11dfb37","outputId":"80b0f233-75c3-4721-88ad-f753727a1667"},"outputs":[{"name":"stdout","output_type":"stream","text":["Problem with file job28.html\n","Problem with file job29.html\n","Problem with file job32.html\n"]}],"source":["# Get names of all files\n","pages = os.listdir()\n","\n","# Loop through html files\n","job_summary = []\n","for page in pages:\n","    try:\n","        job_summary.append(getStuff(page))\n","    except:\n","        print(\"Problem with file\", page)"]},{"cell_type":"code","execution_count":null,"id":"b70a50f3","metadata":{"id":"b70a50f3","outputId":"5a477c2c-7f3a-4add-8b4d-f3defa69e7b1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>jobtitle</th>\n","      <th>employer</th>\n","      <th>place</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Senior Data Scientist</td>\n","      <td>BLS AG</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Data Scientist, Data Analyst</td>\n","      <td>Das Bundesamt für Gesundheit BAG</td>\n","      <td>3097 Liebefeld, BE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Assistant to the Communication and Project Off...</td>\n","      <td>Universität Bern Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Data Engineer (Big Data)</td>\n","      <td>Fincons Group</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Instrument Scientist Space Research / Planetar...</td>\n","      <td>Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Bioinformatician/Computational Biologist 100%</td>\n","      <td>Universität Bern Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Bioinformatician/Computational Biologist 100%</td>\n","      <td>Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Downstream Process Scientist</td>\n","      <td>Hobson Prior</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Scientific Research Associate - R&amp;D (m/f/d)</td>\n","      <td>IDEXX</td>\n","      <td>3097 Liebefeld, BE</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Instrument Scientist Space Research / Planetar...</td>\n","      <td>Universität Bern Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Postdoc (f/m/d)</td>\n","      <td>Insel Gruppe</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Full Professorship in Translational Pharmacolo...</td>\n","      <td>Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Biodata Scientist (w/m/d)</td>\n","      <td>Insel Gruppe</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>PhD</td>\n","      <td>Insel Gruppe</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Senior Data Engineer</td>\n","      <td>mimacom</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Business Intelligence Analyst and Requirements...</td>\n","      <td>Schweizerischer Nationalfonds</td>\n","      <td>3001 Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Security Operations Engineer / DevOps Engineer</td>\n","      <td>AdNovum</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Application Scientist in Process Analysis, Dev...</td>\n","      <td>TOFWERK AG</td>\n","      <td>3600 Thun, BE</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Senior Data Engineer</td>\n","      <td>mimacom ag</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Jun./Prof. Entwickler im Big Data Umfeld</td>\n","      <td>Future IT GmbH</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>IT Business Analyst (m/w/d)</td>\n","      <td>Data Science Jobs</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Data Scientist</td>\n","      <td>Data Science Jobs</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Jun./Prof. Entwickler im Big Data Umfeld</td>\n","      <td>Future IT GmbH</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Security Operations Engineer / DevOps Engineer</td>\n","      <td>AdNovum Informatik</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Downstream Process Scientist</td>\n","      <td>We make it</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>ICT Data Scientist GIS 80 - 100%</td>\n","      <td>Energie Wasser Bern</td>\n","      <td>3001 Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Assistant to the Communication and Project Off...</td>\n","      <td>Universität Bern</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Senior Data Scientist 80%-100%</td>\n","      <td>Data Science Jobs</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>ICT Data Scientist GIS</td>\n","      <td>ewb Energie Wasser Bern</td>\n","      <td>3011 Bern, BE</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Research associate/assistant (f/m/d)</td>\n","      <td>Insel Gruppe</td>\n","      <td>Bern, BE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             jobtitle  \\\n","0                               Senior Data Scientist   \n","1                        Data Scientist, Data Analyst   \n","2   Assistant to the Communication and Project Off...   \n","3                            Data Engineer (Big Data)   \n","4   Instrument Scientist Space Research / Planetar...   \n","5       Bioinformatician/Computational Biologist 100%   \n","6       Bioinformatician/Computational Biologist 100%   \n","7                        Downstream Process Scientist   \n","8         Scientific Research Associate - R&D (m/f/d)   \n","9   Instrument Scientist Space Research / Planetar...   \n","10                                    Postdoc (f/m/d)   \n","11  Full Professorship in Translational Pharmacolo...   \n","12                          Biodata Scientist (w/m/d)   \n","13                                                PhD   \n","14                               Senior Data Engineer   \n","15  Business Intelligence Analyst and Requirements...   \n","16     Security Operations Engineer / DevOps Engineer   \n","17  Application Scientist in Process Analysis, Dev...   \n","18                               Senior Data Engineer   \n","19           Jun./Prof. Entwickler im Big Data Umfeld   \n","20                        IT Business Analyst (m/w/d)   \n","21                                     Data Scientist   \n","22           Jun./Prof. Entwickler im Big Data Umfeld   \n","23     Security Operations Engineer / DevOps Engineer   \n","24                       Downstream Process Scientist   \n","25                   ICT Data Scientist GIS 80 - 100%   \n","26  Assistant to the Communication and Project Off...   \n","27                     Senior Data Scientist 80%-100%   \n","28                             ICT Data Scientist GIS   \n","29               Research associate/assistant (f/m/d)   \n","\n","                             employer               place  \n","0                              BLS AG            Bern, BE  \n","1    Das Bundesamt für Gesundheit BAG  3097 Liebefeld, BE  \n","2   Universität Bern Universität Bern            Bern, BE  \n","3                       Fincons Group            Bern, BE  \n","4                    Universität Bern            Bern, BE  \n","5   Universität Bern Universität Bern            Bern, BE  \n","6                    Universität Bern            Bern, BE  \n","7                        Hobson Prior            Bern, BE  \n","8                               IDEXX  3097 Liebefeld, BE  \n","9   Universität Bern Universität Bern            Bern, BE  \n","10                       Insel Gruppe            Bern, BE  \n","11                   Universität Bern            Bern, BE  \n","12                       Insel Gruppe            Bern, BE  \n","13                       Insel Gruppe            Bern, BE  \n","14                            mimacom            Bern, BE  \n","15      Schweizerischer Nationalfonds       3001 Bern, BE  \n","16                            AdNovum            Bern, BE  \n","17                         TOFWERK AG       3600 Thun, BE  \n","18                         mimacom ag            Bern, BE  \n","19                     Future IT GmbH            Bern, BE  \n","20                  Data Science Jobs            Bern, BE  \n","21                  Data Science Jobs            Bern, BE  \n","22                     Future IT GmbH            Bern, BE  \n","23                 AdNovum Informatik            Bern, BE  \n","24                         We make it            Bern, BE  \n","25                Energie Wasser Bern       3001 Bern, BE  \n","26                   Universität Bern            Bern, BE  \n","27                  Data Science Jobs            Bern, BE  \n","28            ewb Energie Wasser Bern       3011 Bern, BE  \n","29                       Insel Gruppe            Bern, BE  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Covert to pandas dataframe\n","df = pd.DataFrame.from_records(job_summary, columns = [\"jobtitle\", \"employer\", \"place\"])\n","df"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}