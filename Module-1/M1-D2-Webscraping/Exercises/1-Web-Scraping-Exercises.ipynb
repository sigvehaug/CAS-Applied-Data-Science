{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1J5sCNUxmBh0t7MH3cVHColt9WFjca3ze","timestamp":1614455032952}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U3zPuM3zuwFT"},"source":["### Web Scraping##\n","#### CAS Applied Data Science 2025 ####\n","\n","#Exercises#"]},{"cell_type":"markdown","metadata":{"id":"ihjiiXFWCoOL"},"source":["\n","\n","---\n","\n","\n","<font color='violet'>\n","Hints are written in white, so you do not see them immediately. If you highlight them (or double-click on them), they will appear!\n","<font color='white'> I am a hint! :-)\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"U2hSLoyOBc1C"},"source":["## 1. Basic exercises"]},{"cell_type":"markdown","metadata":{"id":"fTXm4BjVXSwH"},"source":["### Exercise 1.1"]},{"cell_type":"markdown","metadata":{"id":"4LigdyQsguzj"},"source":["Import the ``requests`` library, the ``BeautifulSoup`` library and the ``pandas`` library."]},{"cell_type":"code","metadata":{"id":"OP6Rw0trhTGl"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikjlp_1mhiQT"},"source":["Using the ``requests`` library, retrieve the example page (http://farys.org/daten/example-page.html) and assign the response object to a variable named ``exR``. Print out the status code. You will get a  number. What does it mean?"]},{"cell_type":"code","metadata":{"id":"ipgty3mXi5JE"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2upIicTWjvAi"},"source":["Now print out the text of the response."]},{"cell_type":"code","metadata":{"id":"AI3TosUojy1x"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrppfJNNks2D"},"source":["### Exercise 1.2"]},{"cell_type":"markdown","metadata":{"id":"4EvpnIcBlxbK"},"source":["Using ``BeautifulSoup``, parse the text of your response object and assign the result to a variable called ``mySoup``."]},{"cell_type":"code","metadata":{"id":"lBCQksmamxtn"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAMXMHCBmdzu"},"source":["Print the content of your soup object."]},{"cell_type":"code","metadata":{"id":"wcDEhu4QmyfF"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Sn-baMZmzIF"},"source":["Now, try to access the following:\n","1. The ``thead`` element\n","2. All ``p`` elements\n","3. The last ``p`` element\n","4. The text of the ``h1`` element\n","5. The first URL in the document (only the URL!)\n","6. All ``a`` elements within the ``table`` element\n","7. All ``table`` elements of class \"cat_table\"  <font color='violet'> Hint: <font color='white'>  Remember to use class_ instead of class to find elements based on the value of the class attribute!<font color='black'>\n","8. The text of all ``p`` elements (as a list) <font color='violet'> Hint: <font color='white'>  Use a list comprehension! <font color='black'>\n","\n"]},{"cell_type":"code","metadata":{"id":"bc64FjCDo-HQ"},"source":["# 1: The thead element\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZM9YGDWo-B3"},"source":["# 2: All p elements\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QiRdk_To96f"},"source":["# 3: The last p element\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBXHgh79o9zm"},"source":["# 4: The text of the h1 element\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZhL7gtfo9sv"},"source":["# 5: The first URL in the document (only the URL!)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkpR1GiFo9lo"},"source":["# 6: All a elements within the table element\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmFdainGmmNn"},"source":["# 7: All table elements of class \"cat_table\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVWKcYleo9eH"},"source":["# 8: The text of all p elements (as a list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYEvEr7QABjT"},"source":["### Exercise 1.3"]},{"cell_type":"markdown","metadata":{"id":"s3U_Q7V4AF9B"},"source":["Most websites are a bit more complicated than our example page. In this exercise, we will retrieve the Wikipedia page on cats: https://en.wikipedia.org/wiki/Cat"]},{"cell_type":"markdown","metadata":{"id":"cSRj3gIpAdgs"},"source":["Retrieve the page, get the text and convert it to a BeautifulSoup object called ``cats``."]},{"cell_type":"code","metadata":{"id":"MKgkt-GTA3Tw"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y00ikW9ABmSo"},"source":["Go to the page and inspect it (right-click on the different elements and select \"Insepct\" (Element untersuchen)\". Then, try to retrieve the following elements from the page:\n","\n","1. The title of the page (only the text)\n","2. The title header of the page (Cat)\n","3.  All the main headers of the text on the page (Etymology and naming, Taxonomy...)\n","4. All the headers in the text (Etymology and naming, Taxonomy, Evolution, Domestication, Characteristics, Size...)\n"," <font color='violet'> Hint: <font color='white'> These headers are all of the same class.<font color='black'>\n","5. The opening paragraph (\"The cat (Felis catus) is a ...\")\n","6. All the links in the infobox table on the right\n","7. The number of images on the page <font color='violet'> Hint: <font color='white'> Hint: You can use the len() function"]},{"cell_type":"code","metadata":{"id":"1BNAIflGCoQ_"},"source":["# 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RarP03dGDk3J"},"source":["# 2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZn0L62BDuSw"},"source":["# 3\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_m88q-Z5GgnN"},"source":["# 4\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckYb0J_8KoeP"},"source":["# 5\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEolGY--HYr8"},"source":["# 6\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IofJLYJHbgSo"},"source":["# 7\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMbIgP9M_wf_"},"source":["Now try to retrieve the first table on the page and convert it to a Pandas dataframe."]},{"cell_type":"code","metadata":{"id":"HcVwJ5E9ATS-"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zesfc5Otne-8"},"source":["### Exercise 1.4"]},{"cell_type":"markdown","metadata":{"id":"tb69L2RcN0Hn"},"source":["Consider the following list of links to Wikipedia pages on animals:"]},{"cell_type":"code","metadata":{"id":"GavxRwQvN8Rt"},"source":["animals_wiki = [\"https://en.wikipedia.org/wiki/Cat\",\n","                \"https://en.wikipedia.org/wiki/Dog\",\n","                \"https://en.wikipedia.org/wiki/Tiger\",\n","                \"https://en.wikipedia.org/wiki/Panda\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wxV20wlP8ql"},"source":["Write a simple loop that fetches each of these pages and writes the response into a list (it should look like this: ``[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]``)"]},{"cell_type":"code","metadata":{"id":"EeeWVHQ8Nz0X"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRc8cXUEnkDU"},"source":["You would like to have (1) the title header and (2) the number of images of each of these pages. Revise your loop so that this information is retreived from the source code of each page and written into a (nested) list. Which animal page has the most images?"]},{"cell_type":"code","metadata":{"id":"pLIaoPb7njfE"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITCk8WoQTfYW"},"source":["Repetition task: Convert your list into a pandas dataframe."]},{"cell_type":"code","source":[],"metadata":{"id":"qXubF1oTBLcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ehbg7j1uTKNp"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I5fGuENsxWXa"},"source":["## 2. Advanced exercises*"]},{"cell_type":"markdown","metadata":{"id":"ZjVQNxR4daUW"},"source":["### Exercise 2.1"]},{"cell_type":"markdown","metadata":{"id":"a-NbMd9KfEu1"},"source":["Suppose your list of animal links also contains a link to a website that does not exist:"]},{"cell_type":"code","metadata":{"id":"2xiYa5zZfB7e"},"source":["animals_wiki = [\"https://en.wikipedia.org/wiki/Cat\",\n","                \"https://en.wikipedia.org/wiki/Dog\",\n","                \"https://no-such-link-exists.com\",\n","                \"https://en.wikipedia.org/wiki/Panda\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vf7QMF29fJd8"},"source":["Add a ``try-exept`` block to the loop from Exercise 1.4 to prevent your web scraper from crashing when an URL cannot be retrieved. <font color='violet'> Hint: <font color='white'> Use ``continue`` within the ``except`` block to jump back to the beginning of the loop!\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"APvssG-KfLcS"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1es3ky6e9_q"},"source":["### Exercise 2.2"]},{"cell_type":"markdown","metadata":{"id":"QfMgxGeJcw--"},"source":["The Wikipedia page https://en.wikipedia.org/wiki/List_of_cat_breeds contains a list of all cat breeds and the links to the respective Wikipedia pages. You would like to create a dataset about the different cat breeds with information from their Wikipedia pages.\n","\n","In a first step, you will have to retrieve all the links to the respective Wikipedia pages. Retrieve them from the first table on the website and write them into a list. *Note that the table also contains some links you do not want to have included (you only want those to the pages for the different cat breeds). You can use ``CSS`` selectors to specify what links you want to extract. First have a look at the source code of the page to find out how the relevant links can be addressed.*"]},{"cell_type":"code","metadata":{"id":"oFgyXzHxVkSv"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ch3yRDKkf6o3"},"source":["If you inspect your list, you may notice that there are some external URLs (i.e. URLs that do not point to Wikipedia pages). Try to remove them! <font color='violet'> Hint: <font color='white'> Note that you can specify an if-condition within a list comprehension. In the very first tutorial you learned how to check if a string is containend within anotther string."]},{"cell_type":"code","metadata":{"id":"AUihku13baXZ"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66cRQYqNcvov"},"source":["With your cleaned list you can now start to scrape the pages. You would like to retrieve (1) the title header, (2) the number of images, (3) the number of characters of the text of each page, and (4) the text of the introductory paragraph. Try to do so for the **first page only** to develop your code.  Note that you will have to add the top level domain (https://en.wikipedia.org) to your URL!"]},{"cell_type":"code","metadata":{"id":"g6S-P27ojJIR"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdIOFIzYjcs9"},"source":["Now you are ready to build your web scraper. Write a loop the fetches the information from all the pages and writes it into a list. Tipp: Before you loop through the entire list, try looping over the first few elements to check if everything works (running the loop across the whole list may take a while)."]},{"cell_type":"code","metadata":{"id":"S9Gm5IlpoWS3"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6S7NWWYjn6c"},"source":["Create a Pandas dataframe with the information you gathered and inspect it. Which cat has the longest article? Which one has the most images?"]},{"cell_type":"code","metadata":{"id":"ebz0FVu7n0Eg"},"source":[],"execution_count":null,"outputs":[]}]}